{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRider Algorithm\n",
    "Building regression model with TensorFlow's native APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 300\n",
    "L_RATE = 0.01\n",
    "\n",
    "# def imgPreprocess(rgb):\n",
    "#     r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "#     # tgt = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "#     tgt = 0.5 * r + 0.25 * g + 0.25 * b\n",
    "#     return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    \n",
    "    features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                    'image': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64),\n",
    "            })\n",
    "\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    image.set_shape((4800))\n",
    "\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "    return image, label\n",
    "\n",
    "def inputs(file, batch_size, num_epochs):\n",
    "\n",
    "    filename = file + '.tfrecords'\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        \n",
    "        dataset = tf.data.TFRecordDataset(filename)\n",
    "\n",
    "        dataset = dataset.map(decode)\n",
    "        dataset = dataset.map(normalize)\n",
    "\n",
    "        dataset = dataset.shuffle(1000 + 3 * batch_size)\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        x, y = iterator.get_next()\n",
    "        x = tf.reshape(x, [BATCH_SIZE, 14400])\n",
    "        y = tf.reshape(y, [BATCH_SIZE, 1])\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sess = tf.InteractiveSession()\n",
    "x, y = inputs('train', BATCH_SIZE, EPOCHS)\n",
    "print(tf.shape(x))\n",
    "print(tf.shape(y))\n",
    "print(tf.Tensor.get_shape(x))\n",
    "print(tf.Tensor.get_shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ximg = np.array(sess.run(x))\n",
    "print(ximg.shape)\n",
    "ximg = ximg.reshape((BATCH_SIZE, 60, 80, 3))\n",
    "plt.imshow(ximg[0,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, 14400])\n",
    "Y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([14400, 2048])),\n",
    "    'h2': tf.Variable(tf.random_normal([2048, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 1]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([2048])),\n",
    "    'b2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1]))\n",
    "}\n",
    "\n",
    "# Create model\n",
    "def neural_net(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#     logits=logits, labels=Y))\n",
    "loss_op = tf.reduce_mean(tf.losses.mean_squared_error(Y, prediction))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=L_RATE)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.greater(0.250, prediction - Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 128.2500, Training Accuracy= 0.285\n",
      "Step 5, Minibatch Loss= 112.1328, Training Accuracy= 0.297\n",
      "Step 10, Minibatch Loss= 112.1523, Training Accuracy= 0.328\n",
      "Step 15, Minibatch Loss= 118.3281, Training Accuracy= 0.309\n",
      "Step 20, Minibatch Loss= 121.1914, Training Accuracy= 0.305\n",
      "Step 25, Minibatch Loss= 125.1016, Training Accuracy= 0.289\n",
      "Step 30, Minibatch Loss= 141.1406, Training Accuracy= 0.297\n",
      "Step 35, Minibatch Loss= 118.2969, Training Accuracy= 0.324\n",
      "Step 40, Minibatch Loss= 107.1250, Training Accuracy= 0.320\n",
      "Step 45, Minibatch Loss= 130.0742, Training Accuracy= 0.285\n",
      "Step 50, Minibatch Loss= 124.0742, Training Accuracy= 0.301\n",
      "Step 55, Minibatch Loss= 120.6094, Training Accuracy= 0.305\n",
      "Step 60, Minibatch Loss= 113.6523, Training Accuracy= 0.285\n",
      "Step 65, Minibatch Loss= 129.1797, Training Accuracy= 0.328\n",
      "Step 70, Minibatch Loss= 122.9219, Training Accuracy= 0.273\n",
      "Step 75, Minibatch Loss= 123.4336, Training Accuracy= 0.336\n",
      "Step 80, Minibatch Loss= 129.2617, Training Accuracy= 0.309\n",
      "Step 85, Minibatch Loss= 126.2227, Training Accuracy= 0.305\n",
      "Step 90, Minibatch Loss= 120.2812, Training Accuracy= 0.344\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, EPOCHS + 1):\n",
    "        batch_x, batch_y = sess.run(inputs('train', BATCH_SIZE, EPOCHS))\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % 5 == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
