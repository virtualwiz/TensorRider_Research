{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRider Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "from tensorlayer.layers import *\n",
    "LayersConfig.tf_dtype = tf.float16  # tf.float32  tf.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2mono(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    # mono = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    mono = 0.5 * r + 0.25 * g + 0.25 * b\n",
    "    return mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataArrays(iterator):\n",
    "    X = np.zeros(4800)\n",
    "    y = np.zeros(1)\n",
    "    recordCounter = 0;\n",
    "\n",
    "    for string_record in iterator:\n",
    "        recordCounter += 1\n",
    "\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(string_record)\n",
    "        imageString = (example.features.feature['image'].bytes_list.value[0])\n",
    "        label = (example.features.feature['label'].int64_list.value[0])\n",
    "\n",
    "        image = np.fromstring(imageString, dtype=np.uint8)\n",
    "        image = image.reshape((60, 80, 3))\n",
    "        image = rgb2mono(image)\n",
    "        image = image.reshape((4800))\n",
    "        \n",
    "        X = np.append(X,image)\n",
    "        y = np.append(y,label)\n",
    "        if recordCounter % 100 == 0:\n",
    "            print(recordCounter,end = '->')\n",
    "            \n",
    "    X = X.reshape((recordCounter + 1, 60, 80, 1))\n",
    "    y = y.reshape((recordCounter + 1,))\n",
    "    y = np.round(y / 12)\n",
    "    y = y + 3\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIterator = tf.python_io.tf_record_iterator(path=\"train.tfrecords\")\n",
    "valIterator = tf.python_io.tf_record_iterator(path=\"val.tfrecords\")\n",
    "testIterator = tf.python_io.tf_record_iterator(path=\"test.tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecords -> Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train...\n",
      "100->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200->300->400->500->600->700->800->900->1000->1100->1200->1300->1400->1500->1600->1700->1800->1900->2000->2100->2200->2300->2400->2500->2600->2700->2800->2900->3000->3100->3200->3300->3400->3500->3600->3700->3800->3900->4000->4100->4200->4300->4400->4500->4600->4700->4800->4900->5000->5100->5200->5300->5400->5500->5600->5700->5800->5900->6000->6100->6200->6300->6400->6500->6600->6700->6800->6900->7000->7100->7200->7300->7400->7500->7600->7700->7800->7900->8000->8100->8200->8300->8400->8500->8600->8700->8800->8900->9000->9100->9200->9300->9400->9500->9600->9700->9800->9900->10000->10100->10200->10300->10400->10500->10600->10700->10800->10900->11000->11100->11200->11300->11400->11500->11600->11700->11800->11900->12000->12100->12200->12300->12400->12500->12600->12700->12800->12900->13000->13100->13200->13300->13400->13500->\n",
      "Val...\n",
      "100->200->300->400->500->600->700->800->900->1000->1100->1200->1300->1400->1500->1600->1700->1800->1900->2000->2100->2200->2300->2400->2500->2600->2700->\n",
      "Test...\n",
      "100->200->300->400->500->600->700->800->900->1000->1100->1200->1300->1400->1500->1600->1700->1800->1900->2000->2100->2200->2300->2400->2500->2600->2700->2800->2900->3000->3100->3200->3300->3400->3500->"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain...\")\n",
    "X_train, y_train = prepareDataArrays(trainIterator)\n",
    "print(\"\\nVal...\")\n",
    "X_val, y_val = prepareDataArrays(valIterator)\n",
    "print(\"\\nTest...\")\n",
    "X_test, y_test = prepareDataArrays(testIterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays -> NPZ File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"arrays2d.npz\", xtr = X_train, ytr = y_train, xval = X_val, yval = y_val, xt = X_test, yt = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPZ File -> Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "npRecall = np.load(\"arrays2d.npz\")\n",
    "X_train = npRecall[\"xtr\"]\n",
    "y_train = npRecall[\"ytr\"]\n",
    "X_val = npRecall[\"xval\"]\n",
    "y_val = npRecall[\"yval\"]\n",
    "X_test = npRecall[\"xt\"]\n",
    "y_test = npRecall[\"yt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.930e+02, 0.000e+00, 4.565e+03, 0.000e+00, 4.786e+03, 0.000e+00,\n",
       "        3.352e+03, 0.000e+00, 3.960e+02, 3.000e+00]),\n",
       " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAD35JREFUeJzt3V2sXWWdx/HvT4ovgy9FOZKmrVOMjREnGSAnlQmJccCUIsZyIQlmRhvCpDeMwcwkWrxpfCHBGzEmI5mGMlMctRKUQJSIDS9xvOClFQShknaQkZMytqaAMkYN+J+L89TZ4GnPPvTsveU8309ystf6r2ft9TwX7W+vtZ61d6oKSVJ/XjXpDkiSJsMAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0aKgCSPJHk4SQPJtndam9OsivJvvZ6cqsnyZeT7E/yUJKzBt5nU2u/L8mm0QxJkjSMhZwB/G1VnVFV0219C3BHVa0F7mjrABcAa9vfZuBamA0MYCvwHmAdsPVIaEiSxm/Zcey7EXhfW94B3A18qtVvqNlHjO9JsjzJitZ2V1UdBkiyC9gAfONoBzjllFNqzZo1x9FFSerPnj17fllVU/O1GzYACvh+kgL+taq2AadW1VMAVfVUkre2tiuBJwf2nWm1o9VfJMlmZs8ceNvb3sbu3buH7KIkCSDJfw/TbtgAOKeqDrT/5Hcl+emxjj1HrY5Rf3FhNly2AUxPT/tFRZI0IkPdA6iqA+31IHAzs9fwf9Eu7dBeD7bmM8Dqgd1XAQeOUZckTcC8AZDkpCRvOLIMrAd+AtwKHJnJswm4pS3fCnyszQY6G3i2XSq6HVif5OR283d9q0mSJmCYS0CnAjcnOdL+61X1vST3AzcmuQz4OXBxa38b8AFgP/Ab4FKAqjqc5HPA/a3dZ4/cEJYkjV/+nH8PYHp6urwJLEkLk2TPwJT9o/JJYEnqlAEgSZ0yACSpUwaAJHXqeL4KQpq4NVu+O7FjP3H1hRM7trQYPAOQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoHwZYQH4qStBCeAUhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVNDB0CSE5I8kOQ7bf20JPcm2Zfkm0le3eqvaev72/Y1A+9xZas/luT8xR6MJGl4CzkDuALYO7D+BeCaqloLPA1c1uqXAU9X1TuAa1o7kpwOXAK8G9gAfCXJCcfXfUnSyzVUACRZBVwIXNfWA5wL3NSa7AAuassb2zpt+3mt/UZgZ1X9rqp+BuwH1i3GICRJCzfsGcCXgE8Cf2jrbwGeqarn2/oMsLItrwSeBGjbn23t/1ifYx9J0pjNGwBJPggcrKo9g+U5mtY82461z+DxNifZnWT3oUOH5uueJOllGuYM4BzgQ0meAHYye+nnS8DyJMtam1XAgbY8A6wGaNvfBBwerM+xzx9V1baqmq6q6ampqQUPSJI0nHkDoKqurKpVVbWG2Zu4d1bV3wF3AR9uzTYBt7TlW9s6bfudVVWtfkmbJXQasBa4b9FGIklakGXzNzmqTwE7k3weeADY3urbga8m2c/sJ/9LAKrqkSQ3Ao8CzwOXV9ULx3F8SdJxWFAAVNXdwN1t+XHmmMVTVb8FLj7K/lcBVy20k5KkxeeTwJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1atmkOyBpYdZs+e7Ejv3E1RdO7NhafJ4BSFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASerUvAGQ5LVJ7kvy4ySPJPlMq5+W5N4k+5J8M8mrW/01bX1/275m4L2ubPXHkpw/qkFJkuY3zBnA74Bzq+qvgTOADUnOBr4AXFNVa4Gngcta+8uAp6vqHcA1rR1JTgcuAd4NbAC+kuSExRyMJGl48wZAzXqurZ7Y/go4F7ip1XcAF7XljW2dtv28JGn1nVX1u6r6GbAfWLcoo5AkLdhQ9wCSnJDkQeAgsAv4L+CZqnq+NZkBVrbllcCTAG37s8BbButz7CNJGrOhAqCqXqiqM4BVzH5qf9dczdprjrLtaPUXSbI5ye4kuw8dOjRM9yRJL8OCZgFV1TPA3cDZwPIkR35PYBVwoC3PAKsB2vY3AYcH63PsM3iMbVU1XVXTU1NTC+meJGkBhpkFNJVkeVt+HfB+YC9wF/Dh1mwTcEtbvrWt07bfWVXV6pe0WUKnAWuB+xZrIJKkhRnmF8FWADvajJ1XATdW1XeSPArsTPJ54AFge2u/Hfhqkv3MfvK/BKCqHklyI/Ao8DxweVW9sLjDkSQNa94AqKqHgDPnqD/OHLN4quq3wMVHea+rgKsW3k1J0mLzSWBJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlT8wZAktVJ7kqyN8kjSa5o9Tcn2ZVkX3s9udWT5MtJ9id5KMlZA++1qbXfl2TT6IYlSZrPMGcAzwP/XFXvAs4GLk9yOrAFuKOq1gJ3tHWAC4C17W8zcC3MBgawFXgPsA7YeiQ0JEnjN28AVNVTVfWjtvxrYC+wEtgI7GjNdgAXteWNwA016x5geZIVwPnArqo6XFVPA7uADYs6GknS0BZ0DyDJGuBM4F7g1Kp6CmZDAnhra7YSeHJgt5lWO1r9pcfYnGR3kt2HDh1aSPckSQswdAAkeT3wLeATVfWrYzWdo1bHqL+4ULWtqqaranpqamrY7kmSFmioAEhyIrP/+X+tqr7dyr9ol3ZorwdbfQZYPbD7KuDAMeqSpAkYZhZQgO3A3qr64sCmW4EjM3k2AbcM1D/WZgOdDTzbLhHdDqxPcnK7+bu+1SRJE7BsiDbnAB8FHk7yYKt9GrgauDHJZcDPgYvbttuADwD7gd8AlwJU1eEknwPub+0+W1WHF2UUkqQFmzcAquqHzH39HuC8OdoXcPlR3ut64PqFdFCSNBo+CSxJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpU/MGQJLrkxxM8pOB2puT7Eqyr72e3OpJ8uUk+5M8lOSsgX02tfb7kmwazXAkScMa5gzg34ENL6ltAe6oqrXAHW0d4AJgbfvbDFwLs4EBbAXeA6wDth4JDUnSZMwbAFX1A+DwS8obgR1teQdw0UD9hpp1D7A8yQrgfGBXVR2uqqeBXfxpqEiSxujl3gM4taqeAmivb231lcCTA+1mWu1odUnShCz2TeDMUatj1P/0DZLNSXYn2X3o0KFF7Zwk6f+93AD4Rbu0Q3s92OozwOqBdquAA8eo/4mq2lZV01U1PTU19TK7J0maz8sNgFuBIzN5NgG3DNQ/1mYDnQ082y4R3Q6sT3Jyu/m7vtUkSROybL4GSb4BvA84JckMs7N5rgZuTHIZ8HPg4tb8NuADwH7gN8ClAFV1OMnngPtbu89W1UtvLEuSxmjeAKiqjxxl03lztC3g8qO8z/XA9QvqnSRpZHwSWJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUqXl/EOaVbM2W707kuE9cfeFEjitJC+EZgCR1ygCQpE4ZAJLUKQNAkjq1pG8CS1oanNAxGp4BSFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTYw+AJBuSPJZkf5It4z6+JGnWWAMgyQnAvwAXAKcDH0ly+jj7IEmaNe4zgHXA/qp6vKp+D+wENo65D5Ikxh8AK4EnB9ZnWk2SNGapqvEdLLkYOL+q/qGtfxRYV1UfH2izGdjcVt8JPHYchzwF+OVx7P9K09t4wTH3wjEvzF9W1dR8jcb9i2AzwOqB9VXAgcEGVbUN2LYYB0uyu6qmF+O9Xgl6Gy845l445tEY9yWg+4G1SU5L8mrgEuDWMfdBksSYzwCq6vkk/wjcDpwAXF9Vj4yzD5KkWWP/Ufiqug24bUyHW5RLSa8gvY0XHHMvHPMIjPUmsCTpz4dfBSFJnVpyAZDk+iQHk/xk0n0ZlySrk9yVZG+SR5JcMek+jVqS1ya5L8mP25g/M+k+jUOSE5I8kOQ7k+7LuCR5IsnDSR5MsnvS/Rm1JMuT3JTkp+3f9N+M7FhL7RJQkvcCzwE3VNVfTbo/45BkBbCiqn6U5A3AHuCiqnp0wl0bmSQBTqqq55KcCPwQuKKq7plw10YqyT8B08Abq+qDk+7POCR5Apiuqi6eA0iyA/jPqrquzZb8i6p6ZhTHWnJnAFX1A+DwpPsxTlX1VFX9qC3/GtjLEn/CumY911ZPbH9L69PMSyRZBVwIXDfpvmg0krwReC+wHaCqfj+q//xhCQZA75KsAc4E7p1sT0avXQ55EDgI7KqqpT7mLwGfBP4w6Y6MWQHfT7KnfVPAUvZ24BDwb+1S33VJThrVwQyAJSTJ64FvAZ+oql9Nuj+jVlUvVNUZzD5Rvi7Jkr3kl+SDwMGq2jPpvkzAOVV1FrPfInx5u8y7VC0DzgKuraozgf8FRva1+QbAEtGug38L+FpVfXvS/Rmndop8N7Bhwl0ZpXOAD7Xr4TuBc5P8x2S7NB5VdaC9HgRuZvZbhZeqGWBm4Gz2JmYDYSQMgCWg3RDdDuytqi9Ouj/jkGQqyfK2/Drg/cBPJ9ur0amqK6tqVVWtYfYrVO6sqr+fcLdGLslJbWID7VLIemDJzvCrqv8BnkzyzlY6DxjZZI6xPwk8akm+AbwPOCXJDLC1qrZPtlcjdw7wUeDhdk0c4NPtqeulagWwo/3I0KuAG6uqm6mRHTkVuHn2Mw7LgK9X1fcm26WR+zjwtTYD6HHg0lEdaMlNA5UkDcdLQJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKRO/R+4OBd5sjCMlgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train)\n",
    "# plt.hist(y_val)\n",
    "# plt.hist(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "x = tf.placeholder(LayersConfig.tf_dtype, shape=[batch_size, 60, 80, 1])\n",
    "y_ = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "\n",
    "rx = tf.placeholder(LayersConfig.tf_dtype, shape=[1, 60, 80, 1])\n",
    "ry_ = tf.placeholder(tf.int64, shape=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        n = InputLayer(x, name='input')\n",
    "        # cnn\n",
    "        n = Conv2d(n, 32, (5, 5), (1, 1), padding='SAME', name='cnn1')\n",
    "        n = BatchNormLayer(n, act=tf.nn.relu, is_train=is_train, name='bn1')\n",
    "        n = MaxPool2d(n, (2, 2), (2, 2), padding='SAME', name='pool1')\n",
    "        n = Conv2d(n, 64, (5, 5), (1, 1), padding='SAME', name='cnn2')\n",
    "        n = BatchNormLayer(n, act=tf.nn.relu, is_train=is_train, name='bn2')\n",
    "        n = MaxPool2d(n, (2, 2), (2, 2), padding='SAME', name='pool2')\n",
    "        # mlp\n",
    "        n = FlattenLayer(n, name='flatten')\n",
    "        n = DropoutLayer(n, 0.5, True, is_train, name='drop1')\n",
    "        n = DenseLayer(n, 256, act=tf.nn.relu, name='relu1')\n",
    "        n = DropoutLayer(n, 0.5, True, is_train, name='drop2')\n",
    "        n = DenseLayer(n, 7, act=tf.identity, name='output')\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  model/input: (128, 60, 80, 1)\n",
      "[TL] Conv2dLayer model/cnn1: shape:(5, 5, 1, 32) strides:(1, 1, 1, 1) pad:SAME act:identity\n",
      "[TL] BatchNormLayer model/bn1: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "[TL] PoolLayer   model/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "[TL] Conv2dLayer model/cnn2: shape:(5, 5, 32, 64) strides:(1, 1, 1, 1) pad:SAME act:identity\n",
      "[TL] BatchNormLayer model/bn2: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "[TL] PoolLayer   model/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "[TL] FlattenLayer model/flatten: 19200\n",
      "[TL] DropoutLayer model/drop1: keep:0.500000 is_fix:True\n",
      "[TL] DenseLayer  model/relu1: 256 relu\n",
      "[TL] DropoutLayer model/drop2: keep:0.500000 is_fix:True\n",
      "[TL] DenseLayer  model/output: 7 identity\n",
      "[TL] InputLayer  model/input: (128, 60, 80, 1)\n",
      "[TL] Conv2dLayer model/cnn1: shape:(5, 5, 1, 32) strides:(1, 1, 1, 1) pad:SAME act:identity\n",
      "[TL] BatchNormLayer model/bn1: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "[TL] PoolLayer   model/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "[TL] Conv2dLayer model/cnn2: shape:(5, 5, 32, 64) strides:(1, 1, 1, 1) pad:SAME act:identity\n",
      "[TL] BatchNormLayer model/bn2: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "[TL] PoolLayer   model/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "[TL] FlattenLayer model/flatten: 19200\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  model/relu1: 256 relu\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  model/output: 7 identity\n",
      "[TL]   param   0: model/cnn1/W_conv2d:0 (5, 5, 1, 32)      float16_ref\n",
      "[TL]   param   1: model/cnn1/b_conv2d:0 (32,)              float16_ref\n",
      "[TL]   param   2: model/bn1/beta:0     (32,)              float16_ref\n",
      "[TL]   param   3: model/bn1/gamma:0    (32,)              float16_ref\n",
      "[TL]   param   4: model/bn1/moving_mean:0 (32,)              float16_ref\n",
      "[TL]   param   5: model/bn1/moving_variance:0 (32,)              float16_ref\n",
      "[TL]   param   6: model/cnn2/W_conv2d:0 (5, 5, 32, 64)     float16_ref\n",
      "[TL]   param   7: model/cnn2/b_conv2d:0 (64,)              float16_ref\n",
      "[TL]   param   8: model/bn2/beta:0     (64,)              float16_ref\n",
      "[TL]   param   9: model/bn2/gamma:0    (64,)              float16_ref\n",
      "[TL]   param  10: model/bn2/moving_mean:0 (64,)              float16_ref\n",
      "[TL]   param  11: model/bn2/moving_variance:0 (64,)              float16_ref\n",
      "[TL]   param  12: model/relu1/W:0      (19200, 256)       float16_ref\n",
      "[TL]   param  13: model/relu1/b:0      (256,)             float16_ref\n",
      "[TL]   param  14: model/output/W:0     (256, 7)           float16_ref\n",
      "[TL]   param  15: model/output/b:0     (7,)               float16_ref\n",
      "[TL]   num of params: 4969735\n"
     ]
    }
   ],
   "source": [
    "# define inferences\n",
    "net_train = model(x, is_train=True, reuse=False)\n",
    "net_test = model(x, is_train=False, reuse=True)\n",
    "\n",
    "net_train.print_params(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost for training\n",
    "y = net_train.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "\n",
    "# cost and accuracy for evalution\n",
    "y2 = net_test.outputs\n",
    "cost_test = tl.cost.cross_entropy(y2, y_, name='xentropy2')\n",
    "correct_prediction = tf.equal(tf.argmax(y2, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, LayersConfig.tf_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   [*] geting variables with model\n"
     ]
    }
   ],
   "source": [
    "# define the optimizer\n",
    "train_params = tl.layers.get_variables_with_name('model', train_only=True, printable=False)\n",
    "train_op = tf.train.AdamOptimizer(\n",
    "    learning_rate=0.0001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    # epsilon=1e-08,    # for float32 as default\n",
    "    epsilon=1e-4,  # for float16, see https://stackoverflow.com/questions/42064941/tensorflow-float16-support-is-broken\n",
    "    use_locking=False).minimize(cost, var_list=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all variables in the session\n",
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 200 took 15.529488s\n",
      "   train loss: 0.883107\n",
      "   train acc: 0.729805\n",
      "   val loss: 1.049456\n",
      "   val acc: 0.690848\n",
      "Epoch 2 of 200 took 14.059394s\n",
      "   train loss: 0.911870\n",
      "   train acc: 0.634655\n",
      "   val loss: 1.065871\n",
      "   val acc: 0.508557\n",
      "Epoch 3 of 200 took 14.196236s\n",
      "   train loss: 0.872103\n",
      "   train acc: 0.724425\n",
      "   val loss: 0.990676\n",
      "   val acc: 0.681920\n",
      "Epoch 4 of 200 took 14.257980s\n",
      "   train loss: 0.755694\n",
      "   train acc: 0.747420\n",
      "   val loss: 0.864932\n",
      "   val acc: 0.728051\n",
      "Epoch 5 of 200 took 14.339309s\n",
      "   train loss: 0.730639\n",
      "   train acc: 0.748157\n",
      "   val loss: 0.820592\n",
      "   val acc: 0.741443\n",
      "Epoch 6 of 200 took 14.321448s\n",
      "   train loss: 0.680028\n",
      "   train acc: 0.750590\n",
      "   val loss: 0.773786\n",
      "   val acc: 0.734747\n",
      "Epoch 7 of 200 took 14.232432s\n",
      "   train loss: 0.664205\n",
      "   train acc: 0.750295\n",
      "   val loss: 0.761440\n",
      "   val acc: 0.741815\n",
      "Epoch 8 of 200 took 14.365364s\n",
      "   train loss: 0.657890\n",
      "   train acc: 0.753611\n",
      "   val loss: 0.751767\n",
      "   val acc: 0.736607\n",
      "Epoch 9 of 200 took 14.528964s\n",
      "   train loss: 0.637444\n",
      "   train acc: 0.754496\n",
      "   val loss: 0.745536\n",
      "   val acc: 0.741815\n",
      "Epoch 10 of 200 took 14.636242s\n",
      "   train loss: 0.638764\n",
      "   train acc: 0.755675\n",
      "   val loss: 0.740002\n",
      "   val acc: 0.737723\n",
      "Epoch 11 of 200 took 14.771797s\n",
      "   train loss: 0.634678\n",
      "   train acc: 0.755823\n",
      "   val loss: 0.734212\n",
      "   val acc: 0.740327\n",
      "Epoch 12 of 200 took 14.249322s\n",
      "   train loss: 0.625249\n",
      "   train acc: 0.756854\n",
      "   val loss: 0.738816\n",
      "   val acc: 0.738095\n",
      "Epoch 13 of 200 took 14.044475s\n",
      "   train loss: 0.626483\n",
      "   train acc: 0.755896\n",
      "   val loss: 0.729701\n",
      "   val acc: 0.742932\n",
      "Epoch 14 of 200 took 14.096777s\n",
      "   train loss: 0.619247\n",
      "   train acc: 0.756781\n",
      "   val loss: 0.732585\n",
      "   val acc: 0.738467\n",
      "Epoch 15 of 200 took 14.153167s\n",
      "   train loss: 0.625415\n",
      "   train acc: 0.758107\n",
      "   val loss: 0.733468\n",
      "   val acc: 0.742932\n",
      "Epoch 16 of 200 took 14.130511s\n",
      "   train loss: 0.616287\n",
      "   train acc: 0.758034\n",
      "   val loss: 0.731910\n",
      "   val acc: 0.745164\n",
      "Epoch 17 of 200 took 14.251349s\n",
      "   train loss: 0.618784\n",
      "   train acc: 0.758328\n",
      "   val loss: 0.732538\n",
      "   val acc: 0.741071\n",
      "Epoch 18 of 200 took 14.264528s\n",
      "   train loss: 0.618604\n",
      "   train acc: 0.758697\n",
      "   val loss: 0.746815\n",
      "   val acc: 0.735119\n",
      "Epoch 19 of 200 took 14.323949s\n",
      "   train loss: 0.619030\n",
      "   train acc: 0.759287\n",
      "   val loss: 0.738793\n",
      "   val acc: 0.739583\n",
      "Epoch 20 of 200 took 14.029545s\n",
      "   train loss: 0.608755\n",
      "   train acc: 0.758476\n",
      "   val loss: 0.739118\n",
      "   val acc: 0.740327\n",
      "Epoch 21 of 200 took 14.016607s\n",
      "   train loss: 0.611439\n",
      "   train acc: 0.757591\n",
      "   val loss: 0.741722\n",
      "   val acc: 0.738839\n",
      "Epoch 22 of 200 took 14.090206s\n",
      "   train loss: 0.614502\n",
      "   train acc: 0.762751\n",
      "   val loss: 0.728957\n",
      "   val acc: 0.742188\n",
      "Epoch 23 of 200 took 14.244913s\n",
      "   train loss: 0.608744\n",
      "   train acc: 0.762751\n",
      "   val loss: 0.717029\n",
      "   val acc: 0.746280\n",
      "Epoch 24 of 200 took 14.206168s\n",
      "   train loss: 0.606531\n",
      "   train acc: 0.764077\n",
      "   val loss: 0.734398\n",
      "   val acc: 0.738467\n",
      "Epoch 25 of 200 took 14.077589s\n",
      "   train loss: 0.613300\n",
      "   train acc: 0.765920\n",
      "   val loss: 0.739165\n",
      "   val acc: 0.738839\n",
      "Epoch 26 of 200 took 14.025306s\n",
      "   train loss: 0.602557\n",
      "   train acc: 0.761940\n",
      "   val loss: 0.744559\n",
      "   val acc: 0.743676\n",
      "Epoch 27 of 200 took 14.115916s\n",
      "   train loss: 0.597656\n",
      "   train acc: 0.764004\n",
      "   val loss: 0.733375\n",
      "   val acc: 0.744048\n",
      "Epoch 28 of 200 took 14.304705s\n",
      "   train loss: 0.604471\n",
      "   train acc: 0.766509\n",
      "   val loss: 0.741281\n",
      "   val acc: 0.740327\n",
      "Epoch 29 of 200 took 14.298895s\n",
      "   train loss: 0.603988\n",
      "   train acc: 0.763340\n",
      "   val loss: 0.742746\n",
      "   val acc: 0.745536\n",
      "Epoch 30 of 200 took 14.400591s\n",
      "   train loss: 0.597560\n",
      "   train acc: 0.764151\n",
      "   val loss: 0.747814\n",
      "   val acc: 0.745536\n",
      "Epoch 31 of 200 took 14.530031s\n",
      "   train loss: 0.601468\n",
      "   train acc: 0.767394\n",
      "   val loss: 0.716820\n",
      "   val acc: 0.749628\n",
      "Epoch 32 of 200 took 14.535643s\n",
      "   train loss: 0.597177\n",
      "   train acc: 0.767025\n",
      "   val loss: 0.734747\n",
      "   val acc: 0.748140\n",
      "Epoch 33 of 200 took 14.525474s\n",
      "   train loss: 0.591903\n",
      "   train acc: 0.766288\n",
      "   val loss: 0.744327\n",
      "   val acc: 0.742188\n",
      "Epoch 34 of 200 took 14.698375s\n",
      "   train loss: 0.594867\n",
      "   train acc: 0.768426\n",
      "   val loss: 0.740258\n",
      "   val acc: 0.747024\n",
      "Epoch 35 of 200 took 14.492692s\n",
      "   train loss: 0.589298\n",
      "   train acc: 0.767615\n",
      "   val loss: 0.728097\n",
      "   val acc: 0.745536\n",
      "Epoch 36 of 200 took 14.542358s\n",
      "   train loss: 0.589802\n",
      "   train acc: 0.769826\n",
      "   val loss: 0.732120\n",
      "   val acc: 0.742932\n",
      "Epoch 37 of 200 took 14.516414s\n",
      "   train loss: 0.589973\n",
      "   train acc: 0.769310\n",
      "   val loss: 0.746675\n",
      "   val acc: 0.738839\n",
      "Epoch 38 of 200 took 14.518229s\n",
      "   train loss: 0.584963\n",
      "   train acc: 0.771005\n",
      "   val loss: 0.739049\n",
      "   val acc: 0.748884\n",
      "Epoch 39 of 200 took 14.378214s\n",
      "   train loss: 0.580963\n",
      "   train acc: 0.769752\n",
      "   val loss: 0.743466\n",
      "   val acc: 0.747768\n",
      "Epoch 40 of 200 took 14.471616s\n",
      "   train loss: 0.579369\n",
      "   train acc: 0.772479\n",
      "   val loss: 0.740955\n",
      "   val acc: 0.746652\n",
      "Epoch 41 of 200 took 14.502467s\n",
      "   train loss: 0.585288\n",
      "   train acc: 0.772258\n",
      "   val loss: 0.733910\n",
      "   val acc: 0.740327\n",
      "Epoch 42 of 200 took 14.260955s\n",
      "   train loss: 0.586230\n",
      "   train acc: 0.773364\n",
      "   val loss: 0.735259\n",
      "   val acc: 0.744420\n",
      "Epoch 43 of 200 took 14.239816s\n",
      "   train loss: 0.578537\n",
      "   train acc: 0.773953\n",
      "   val loss: 0.744699\n",
      "   val acc: 0.743676\n",
      "Epoch 44 of 200 took 14.328217s\n",
      "   train loss: 0.578977\n",
      "   train acc: 0.774248\n",
      "   val loss: 0.731562\n",
      "   val acc: 0.750000\n",
      "Epoch 45 of 200 took 14.259461s\n",
      "   train loss: 0.578857\n",
      "   train acc: 0.774838\n",
      "   val loss: 0.736351\n",
      "   val acc: 0.748884\n",
      "Epoch 46 of 200 took 14.297986s\n",
      "   train loss: 0.571830\n",
      "   train acc: 0.772922\n",
      "   val loss: 0.745652\n",
      "   val acc: 0.739955\n",
      "Epoch 47 of 200 took 14.530560s\n",
      "   train loss: 0.574553\n",
      "   train acc: 0.775943\n",
      "   val loss: 0.744768\n",
      "   val acc: 0.750000\n",
      "Epoch 48 of 200 took 14.518329s\n",
      "   train loss: 0.575916\n",
      "   train acc: 0.775575\n",
      "   val loss: 0.737374\n",
      "   val acc: 0.751116\n",
      "Epoch 49 of 200 took 14.604310s\n",
      "   train loss: 0.583544\n",
      "   train acc: 0.770268\n",
      "   val loss: 0.773600\n",
      "   val acc: 0.741071\n",
      "Epoch 50 of 200 took 14.577950s\n",
      "   train loss: 0.568650\n",
      "   train acc: 0.775206\n",
      "   val loss: 0.741978\n",
      "   val acc: 0.748140\n",
      "Epoch 51 of 200 took 14.522693s\n",
      "   train loss: 0.570545\n",
      "   train acc: 0.774912\n",
      "   val loss: 0.739723\n",
      "   val acc: 0.749256\n",
      "Epoch 52 of 200 took 14.633339s\n",
      "   train loss: 0.569794\n",
      "   train acc: 0.777417\n",
      "   val loss: 0.746047\n",
      "   val acc: 0.746280\n",
      "Epoch 53 of 200 took 14.618174s\n",
      "   train loss: 0.566883\n",
      "   train acc: 0.777786\n",
      "   val loss: 0.735235\n",
      "   val acc: 0.752976\n",
      "Epoch 54 of 200 took 14.712782s\n",
      "   train loss: 0.591751\n",
      "   train acc: 0.776386\n",
      "   val loss: 0.754627\n",
      "   val acc: 0.736607\n",
      "Epoch 55 of 200 took 14.314247s\n",
      "   train loss: 0.565731\n",
      "   train acc: 0.778228\n",
      "   val loss: 0.770043\n",
      "   val acc: 0.750000\n",
      "Epoch 56 of 200 took 14.498792s\n",
      "   train loss: 0.570669\n",
      "   train acc: 0.777565\n",
      "   val loss: 0.743978\n",
      "   val acc: 0.744048\n",
      "Epoch 57 of 200 took 14.569088s\n",
      "   train loss: 0.566330\n",
      "   train acc: 0.779186\n",
      "   val loss: 0.732143\n",
      "   val acc: 0.753348\n",
      "Epoch 58 of 200 took 14.454667s\n",
      "   train loss: 0.556244\n",
      "   train acc: 0.780071\n",
      "   val loss: 0.743571\n",
      "   val acc: 0.752976\n",
      "Epoch 59 of 200 took 14.377835s\n",
      "   train loss: 0.563507\n",
      "   train acc: 0.777196\n",
      "   val loss: 0.747791\n",
      "   val acc: 0.748884\n",
      "Epoch 60 of 200 took 14.390970s\n",
      "   train loss: 0.559280\n",
      "   train acc: 0.781471\n",
      "   val loss: 0.737677\n",
      "   val acc: 0.749256\n",
      "Epoch 61 of 200 took 14.323639s\n",
      "   train loss: 0.562394\n",
      "   train acc: 0.776165\n",
      "   val loss: 0.757394\n",
      "   val acc: 0.746280\n",
      "Epoch 62 of 200 took 14.355260s\n",
      "   train loss: 0.552617\n",
      "   train acc: 0.779776\n",
      "   val loss: 0.749698\n",
      "   val acc: 0.751116\n",
      "Epoch 63 of 200 took 14.580458s\n",
      "   train loss: 0.566858\n",
      "   train acc: 0.773511\n",
      "   val loss: 0.775763\n",
      "   val acc: 0.746652\n",
      "Epoch 64 of 200 took 14.569807s\n",
      "   train loss: 0.567408\n",
      "   train acc: 0.783682\n",
      "   val loss: 0.727597\n",
      "   val acc: 0.747024\n",
      "Epoch 65 of 200 took 14.450840s\n",
      "   train loss: 0.549194\n",
      "   train acc: 0.782577\n",
      "   val loss: 0.748861\n",
      "   val acc: 0.754836\n",
      "Epoch 66 of 200 took 14.506132s\n",
      "   train loss: 0.544556\n",
      "   train acc: 0.782061\n",
      "   val loss: 0.769648\n",
      "   val acc: 0.746652\n",
      "Epoch 67 of 200 took 14.316939s\n",
      "   train loss: 0.540345\n",
      "   train acc: 0.783166\n",
      "   val loss: 0.754371\n",
      "   val acc: 0.749256\n",
      "Epoch 68 of 200 took 14.264499s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train loss: 0.540594\n",
      "   train acc: 0.784051\n",
      "   val loss: 0.745117\n",
      "   val acc: 0.747768\n",
      "Epoch 69 of 200 took 14.424144s\n",
      "   train loss: 0.539228\n",
      "   train acc: 0.783977\n",
      "   val loss: 0.761602\n",
      "   val acc: 0.755580\n",
      "Epoch 70 of 200 took 14.503065s\n",
      "   train loss: 0.558089\n",
      "   train acc: 0.780881\n",
      "   val loss: 0.757022\n",
      "   val acc: 0.736607\n",
      "Epoch 71 of 200 took 14.712161s\n",
      "   train loss: 0.532789\n",
      "   train acc: 0.788694\n",
      "   val loss: 0.740234\n",
      "   val acc: 0.753348\n",
      "Epoch 72 of 200 took 14.492379s\n",
      "   train loss: 0.553211\n",
      "   train acc: 0.785598\n",
      "   val loss: 0.736956\n",
      "   val acc: 0.746280\n",
      "Epoch 73 of 200 took 14.431555s\n",
      "   train loss: 0.526678\n",
      "   train acc: 0.787294\n",
      "   val loss: 0.739397\n",
      "   val acc: 0.751488\n",
      "Epoch 74 of 200 took 14.390351s\n",
      "   train loss: 0.536071\n",
      "   train acc: 0.785156\n",
      "   val loss: 0.736700\n",
      "   val acc: 0.755208\n",
      "Epoch 75 of 200 took 14.476315s\n",
      "   train loss: 0.531609\n",
      "   train acc: 0.785820\n",
      "   val loss: 0.749628\n",
      "   val acc: 0.757068\n",
      "Epoch 76 of 200 took 14.480296s\n",
      "   train loss: 0.535080\n",
      "   train acc: 0.787736\n",
      "   val loss: 0.731143\n",
      "   val acc: 0.753720\n",
      "Epoch 77 of 200 took 14.529759s\n",
      "   train loss: 0.516894\n",
      "   train acc: 0.794885\n",
      "   val loss: 0.716053\n",
      "   val acc: 0.757812\n",
      "Epoch 78 of 200 took 14.627906s\n",
      "   train loss: 0.519402\n",
      "   train acc: 0.792379\n",
      "   val loss: 0.717006\n",
      "   val acc: 0.758929\n",
      "Epoch 79 of 200 took 14.376181s\n",
      "   train loss: 0.510526\n",
      "   train acc: 0.797391\n",
      "   val loss: 0.714193\n",
      "   val acc: 0.752976\n",
      "Epoch 80 of 200 took 14.339080s\n",
      "   train loss: 0.507253\n",
      "   train acc: 0.794885\n",
      "   val loss: 0.732375\n",
      "   val acc: 0.757440\n",
      "Epoch 81 of 200 took 14.471594s\n",
      "   train loss: 0.551901\n",
      "   train acc: 0.791421\n",
      "   val loss: 0.729446\n",
      "   val acc: 0.745908\n",
      "Epoch 82 of 200 took 14.450437s\n",
      "   train loss: 0.509029\n",
      "   train acc: 0.797022\n",
      "   val loss: 0.728016\n",
      "   val acc: 0.758929\n",
      "Epoch 83 of 200 took 14.342100s\n",
      "   train loss: 0.493240\n",
      "   train acc: 0.804835\n",
      "   val loss: 0.697684\n",
      "   val acc: 0.759673\n",
      "Epoch 84 of 200 took 14.410326s\n",
      "   train loss: 0.505366\n",
      "   train acc: 0.803877\n",
      "   val loss: 0.694568\n",
      "   val acc: 0.764509\n",
      "Epoch 85 of 200 took 14.529457s\n",
      "   train loss: 0.501492\n",
      "   train acc: 0.800634\n",
      "   val loss: 0.702567\n",
      "   val acc: 0.760417\n",
      "Epoch 86 of 200 took 14.561063s\n",
      "   train loss: 0.484313\n",
      "   train acc: 0.804688\n",
      "   val loss: 0.687070\n",
      "   val acc: 0.768973\n",
      "Epoch 87 of 200 took 14.578031s\n",
      "   train loss: 0.484400\n",
      "   train acc: 0.802919\n",
      "   val loss: 0.695731\n",
      "   val acc: 0.766369\n",
      "Epoch 88 of 200 took 14.084870s\n",
      "   train loss: 0.504418\n",
      "   train acc: 0.802624\n",
      "   val loss: 0.708357\n",
      "   val acc: 0.752976\n",
      "Epoch 89 of 200 took 14.191746s\n",
      "   train loss: 0.498579\n",
      "   train acc: 0.804761\n",
      "   val loss: 0.699591\n",
      "   val acc: 0.757440\n",
      "Epoch 90 of 200 took 14.046204s\n",
      "   train loss: 0.483677\n",
      "   train acc: 0.811689\n",
      "   val loss: 0.712216\n",
      "   val acc: 0.768229\n",
      "Epoch 91 of 200 took 14.205044s\n",
      "   train loss: 0.497701\n",
      "   train acc: 0.807267\n",
      "   val loss: 0.697847\n",
      "   val acc: 0.751488\n",
      "Epoch 92 of 200 took 14.158779s\n",
      "   train loss: 0.466392\n",
      "   train acc: 0.809994\n",
      "   val loss: 0.692232\n",
      "   val acc: 0.767485\n",
      "Epoch 93 of 200 took 14.152940s\n",
      "   train loss: 0.452556\n",
      "   train acc: 0.816775\n",
      "   val loss: 0.713774\n",
      "   val acc: 0.764509\n",
      "Epoch 94 of 200 took 14.248471s\n",
      "   train loss: 0.465968\n",
      "   train acc: 0.810510\n",
      "   val loss: 0.686244\n",
      "   val acc: 0.768601\n",
      "Epoch 95 of 200 took 14.368882s\n",
      "   train loss: 0.461663\n",
      "   train acc: 0.811689\n",
      "   val loss: 0.684512\n",
      "   val acc: 0.773438\n",
      "Epoch 96 of 200 took 14.500242s\n",
      "   train loss: 0.460917\n",
      "   train acc: 0.817880\n",
      "   val loss: 0.661133\n",
      "   val acc: 0.766369\n",
      "Epoch 97 of 200 took 14.526076s\n",
      "   train loss: 0.456566\n",
      "   train acc: 0.817070\n",
      "   val loss: 0.690837\n",
      "   val acc: 0.765625\n",
      "Epoch 98 of 200 took 14.165817s\n",
      "   train loss: 0.466495\n",
      "   train acc: 0.816848\n",
      "   val loss: 0.665318\n",
      "   val acc: 0.768601\n",
      "Epoch 99 of 200 took 14.406509s\n",
      "   train loss: 0.477343\n",
      "   train acc: 0.812426\n",
      "   val loss: 0.689779\n",
      "   val acc: 0.749256\n",
      "Epoch 100 of 200 took 14.278858s\n",
      "   train loss: 0.457176\n",
      "   train acc: 0.819428\n",
      "   val loss: 0.729074\n",
      "   val acc: 0.753720\n",
      "Epoch 101 of 200 took 14.245789s\n",
      "   train loss: 0.452234\n",
      "   train acc: 0.820755\n",
      "   val loss: 0.677223\n",
      "   val acc: 0.758929\n",
      "Epoch 102 of 200 took 14.345891s\n",
      "   train loss: 0.477187\n",
      "   train acc: 0.819428\n",
      "   val loss: 0.666318\n",
      "   val acc: 0.753720\n",
      "Epoch 103 of 200 took 14.396148s\n",
      "   train loss: 0.451038\n",
      "   train acc: 0.819207\n",
      "   val loss: 0.702265\n",
      "   val acc: 0.762649\n",
      "Epoch 104 of 200 took 14.415962s\n",
      "   train loss: 0.437270\n",
      "   train acc: 0.825767\n",
      "   val loss: 0.672061\n",
      "   val acc: 0.760417\n",
      "Epoch 105 of 200 took 14.532151s\n",
      "   train loss: 0.442113\n",
      "   train acc: 0.820018\n",
      "   val loss: 0.688593\n",
      "   val acc: 0.755952\n",
      "Epoch 106 of 200 took 14.465245s\n",
      "   train loss: 0.434252\n",
      "   train acc: 0.826577\n",
      "   val loss: 0.647275\n",
      "   val acc: 0.774926\n",
      "Epoch 107 of 200 took 14.589269s\n",
      "   train loss: 0.434888\n",
      "   train acc: 0.827462\n",
      "   val loss: 0.672561\n",
      "   val acc: 0.765625\n",
      "Epoch 108 of 200 took 14.589932s\n",
      "   train loss: 0.435740\n",
      "   train acc: 0.824514\n",
      "   val loss: 0.693871\n",
      "   val acc: 0.762277\n",
      "Epoch 109 of 200 took 14.648198s\n",
      "   train loss: 0.441118\n",
      "   train acc: 0.829304\n",
      "   val loss: 0.663690\n",
      "   val acc: 0.769717\n",
      "Epoch 110 of 200 took 14.297129s\n",
      "   train loss: 0.444329\n",
      "   train acc: 0.827388\n",
      "   val loss: 0.696568\n",
      "   val acc: 0.752232\n",
      "Epoch 111 of 200 took 14.309177s\n",
      "   train loss: 0.434720\n",
      "   train acc: 0.829746\n",
      "   val loss: 0.704102\n",
      "   val acc: 0.748140\n",
      "Epoch 112 of 200 took 14.294493s\n",
      "   train loss: 0.444251\n",
      "   train acc: 0.830999\n",
      "   val loss: 0.666132\n",
      "   val acc: 0.754092\n",
      "Epoch 113 of 200 took 14.184767s\n",
      "   train loss: 0.438951\n",
      "   train acc: 0.823187\n",
      "   val loss: 0.722656\n",
      "   val acc: 0.729539\n",
      "Epoch 114 of 200 took 14.327046s\n",
      "   train loss: 0.431756\n",
      "   train acc: 0.829746\n",
      "   val loss: 0.681734\n",
      "   val acc: 0.758185\n",
      "Epoch 115 of 200 took 14.384365s\n",
      "   train loss: 0.424669\n",
      "   train acc: 0.835643\n",
      "   val loss: 0.666167\n",
      "   val acc: 0.755580\n",
      "Epoch 116 of 200 took 14.489893s\n",
      "   train loss: 0.427440\n",
      "   train acc: 0.828788\n",
      "   val loss: 0.713565\n",
      "   val acc: 0.742560\n",
      "Epoch 117 of 200 took 14.311877s\n",
      "   train loss: 0.434764\n",
      "   train acc: 0.831884\n",
      "   val loss: 0.706334\n",
      "   val acc: 0.727307\n",
      "Epoch 118 of 200 took 14.171668s\n",
      "   train loss: 0.456695\n",
      "   train acc: 0.820091\n",
      "   val loss: 0.744815\n",
      "   val acc: 0.711310\n",
      "Epoch 119 of 200 took 14.253074s\n",
      "   train loss: 0.457863\n",
      "   train acc: 0.828788\n",
      "   val loss: 0.676084\n",
      "   val acc: 0.733259\n",
      "Epoch 120 of 200 took 14.085625s\n",
      "   train loss: 0.438255\n",
      "   train acc: 0.824587\n",
      "   val loss: 0.740932\n",
      "   val acc: 0.750744\n",
      "Epoch 121 of 200 took 14.192383s\n",
      "   train loss: 0.432051\n",
      "   train acc: 0.827388\n",
      "   val loss: 0.769368\n",
      "   val acc: 0.719122\n",
      "Epoch 122 of 200 took 14.163323s\n",
      "   train loss: 0.440637\n",
      "   train acc: 0.826577\n",
      "   val loss: 0.676967\n",
      "   val acc: 0.747024\n",
      "Epoch 123 of 200 took 14.152617s\n",
      "   train loss: 0.416009\n",
      "   train acc: 0.833358\n",
      "   val loss: 0.695196\n",
      "   val acc: 0.761161\n",
      "Epoch 124 of 200 took 14.229213s\n",
      "   train loss: 0.435720\n",
      "   train acc: 0.831515\n",
      "   val loss: 0.727888\n",
      "   val acc: 0.762649\n",
      "Epoch 125 of 200 took 14.414589s\n",
      "   train loss: 0.415675\n",
      "   train acc: 0.838959\n",
      "   val loss: 0.666748\n",
      "   val acc: 0.763765\n",
      "Epoch 126 of 200 took 14.102140s\n",
      "   train loss: 0.445554\n",
      "   train acc: 0.832695\n",
      "   val loss: 0.639660\n",
      "   val acc: 0.754836\n",
      "Epoch 127 of 200 took 14.140422s\n",
      "   train loss: 0.419996\n",
      "   train acc: 0.833505\n",
      "   val loss: 0.736863\n",
      "   val acc: 0.735863\n",
      "Epoch 128 of 200 took 14.132622s\n",
      "   train loss: 0.459286\n",
      "   train acc: 0.822892\n",
      "   val loss: 0.662923\n",
      "   val acc: 0.769345\n",
      "Epoch 129 of 200 took 14.097018s\n",
      "   train loss: 0.413441\n",
      "   train acc: 0.834906\n",
      "   val loss: 0.740281\n",
      "   val acc: 0.739583\n",
      "Epoch 130 of 200 took 14.089932s\n",
      "   train loss: 0.430664\n",
      "   train acc: 0.835422\n",
      "   val loss: 0.674130\n",
      "   val acc: 0.748140\n",
      "Epoch 131 of 200 took 14.177711s\n",
      "   train loss: 0.416529\n",
      "   train acc: 0.834463\n",
      "   val loss: 0.751302\n",
      "   val acc: 0.740327\n",
      "Epoch 132 of 200 took 14.302990s\n",
      "   train loss: 0.436933\n",
      "   train acc: 0.825693\n",
      "   val loss: 0.756813\n",
      "   val acc: 0.689732\n",
      "Epoch 133 of 200 took 14.316907s\n",
      "   train loss: 0.420516\n",
      "   train acc: 0.839549\n",
      "   val loss: 0.704497\n",
      "   val acc: 0.737351\n",
      "Epoch 134 of 200 took 14.156564s\n",
      "   train loss: 0.426118\n",
      "   train acc: 0.831663\n",
      "   val loss: 0.671863\n",
      "   val acc: 0.757068\n",
      "Epoch 135 of 200 took 14.074963s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train loss: 0.431542\n",
      "   train acc: 0.830999\n",
      "   val loss: 0.676502\n",
      "   val acc: 0.733259\n",
      "Epoch 136 of 200 took 14.090227s\n",
      "   train loss: 0.425399\n",
      "   train acc: 0.831810\n",
      "   val loss: 0.643590\n",
      "   val acc: 0.774554\n",
      "Epoch 137 of 200 took 14.254581s\n",
      "   train loss: 0.416260\n",
      "   train acc: 0.832621\n",
      "   val loss: 0.707357\n",
      "   val acc: 0.767485\n",
      "Epoch 138 of 200 took 14.228052s\n",
      "   train loss: 0.424425\n",
      "   train acc: 0.834242\n",
      "   val loss: 0.691848\n",
      "   val acc: 0.758929\n",
      "Epoch 139 of 200 took 14.285753s\n",
      "   train loss: 0.412307\n",
      "   train acc: 0.837780\n",
      "   val loss: 0.641288\n",
      "   val acc: 0.765253\n",
      "Epoch 140 of 200 took 14.230137s\n",
      "   train loss: 0.417881\n",
      "   train acc: 0.830336\n",
      "   val loss: 0.703358\n",
      "   val acc: 0.755580\n",
      "Epoch 141 of 200 took 14.060388s\n",
      "   train loss: 0.451381\n",
      "   train acc: 0.829231\n",
      "   val loss: 0.644624\n",
      "   val acc: 0.756696\n",
      "Epoch 142 of 200 took 14.042707s\n",
      "   train loss: 0.402245\n",
      "   train acc: 0.840949\n",
      "   val loss: 0.723110\n",
      "   val acc: 0.739211\n",
      "Epoch 143 of 200 took 14.207147s\n",
      "   train loss: 0.425215\n",
      "   train acc: 0.833874\n",
      "   val loss: 0.699382\n",
      "   val acc: 0.771949\n",
      "Epoch 144 of 200 took 14.182869s\n",
      "   train loss: 0.411649\n",
      "   train acc: 0.836085\n",
      "   val loss: 0.712565\n",
      "   val acc: 0.750744\n",
      "Epoch 145 of 200 took 14.006581s\n",
      "   train loss: 0.423186\n",
      "   train acc: 0.833284\n",
      "   val loss: 0.659401\n",
      "   val acc: 0.755580\n",
      "Epoch 146 of 200 took 14.034011s\n",
      "   train loss: 0.415288\n",
      "   train acc: 0.832547\n",
      "   val loss: 0.733247\n",
      "   val acc: 0.752976\n",
      "Epoch 147 of 200 took 14.115468s\n",
      "   train loss: 0.406803\n",
      "   train acc: 0.844119\n",
      "   val loss: 0.677920\n",
      "   val acc: 0.757440\n",
      "Epoch 148 of 200 took 14.123699s\n",
      "   train loss: 0.423040\n",
      "   train acc: 0.841539\n",
      "   val loss: 0.621524\n",
      "   val acc: 0.767113\n",
      "Epoch 149 of 200 took 14.048676s\n",
      "   train loss: 0.415046\n",
      "   train acc: 0.836232\n",
      "   val loss: 0.697428\n",
      "   val acc: 0.740327\n",
      "Epoch 150 of 200 took 14.202422s\n",
      "   train loss: 0.408316\n",
      "   train acc: 0.840286\n",
      "   val loss: 0.704915\n",
      "   val acc: 0.738467\n",
      "Epoch 151 of 200 took 14.161669s\n",
      "   train loss: 0.413358\n",
      "   train acc: 0.841907\n",
      "   val loss: 0.692208\n",
      "   val acc: 0.741071\n",
      "Epoch 152 of 200 took 14.156662s\n",
      "   train loss: 0.396781\n",
      "   train acc: 0.845814\n",
      "   val loss: 0.714100\n",
      "   val acc: 0.743676\n",
      "Epoch 153 of 200 took 14.255664s\n",
      "   train loss: 0.421207\n",
      "   train acc: 0.836969\n",
      "   val loss: 0.667736\n",
      "   val acc: 0.736979\n",
      "Epoch 154 of 200 took 14.085398s\n",
      "   train loss: 0.401471\n",
      "   train acc: 0.846551\n",
      "   val loss: 0.750430\n",
      "   val acc: 0.755580\n",
      "Epoch 155 of 200 took 14.182178s\n",
      "   train loss: 0.405978\n",
      "   train acc: 0.845371\n",
      "   val loss: 0.681850\n",
      "   val acc: 0.740327\n",
      "Epoch 156 of 200 took 14.140587s\n",
      "   train loss: 0.408390\n",
      "   train acc: 0.845003\n",
      "   val loss: 0.655773\n",
      "   val acc: 0.752976\n",
      "Epoch 157 of 200 took 14.179131s\n",
      "   train loss: 0.411128\n",
      "   train acc: 0.840654\n",
      "   val loss: 0.664830\n",
      "   val acc: 0.751116\n",
      "Epoch 158 of 200 took 14.138144s\n",
      "   train loss: 0.409470\n",
      "   train acc: 0.840139\n",
      "   val loss: 0.678292\n",
      "   val acc: 0.741443\n",
      "Epoch 159 of 200 took 14.162107s\n",
      "   train loss: 0.417918\n",
      "   train acc: 0.837043\n",
      "   val loss: 0.676386\n",
      "   val acc: 0.747024\n",
      "Epoch 160 of 200 took 14.104952s\n",
      "   train loss: 0.396922\n",
      "   train acc: 0.845961\n",
      "   val loss: 0.700916\n",
      "   val acc: 0.751860\n",
      "Epoch 161 of 200 took 14.122517s\n",
      "   train loss: 0.410083\n",
      "   train acc: 0.839917\n",
      "   val loss: 0.681187\n",
      "   val acc: 0.748140\n",
      "Epoch 162 of 200 took 14.106760s\n",
      "   train loss: 0.398426\n",
      "   train acc: 0.841170\n",
      "   val loss: 0.701102\n",
      "   val acc: 0.764137\n",
      "Epoch 163 of 200 took 14.011483s\n",
      "   train loss: 0.415797\n",
      "   train acc: 0.840581\n",
      "   val loss: 0.642008\n",
      "   val acc: 0.765997\n",
      "Epoch 164 of 200 took 14.114785s\n",
      "   train loss: 0.451803\n",
      "   train acc: 0.827978\n",
      "   val loss: 0.715053\n",
      "   val acc: 0.706101\n",
      "Epoch 165 of 200 took 14.085972s\n",
      "   train loss: 0.419751\n",
      "   train acc: 0.837633\n",
      "   val loss: 0.666853\n",
      "   val acc: 0.770089\n",
      "Epoch 166 of 200 took 14.110390s\n",
      "   train loss: 0.397735\n",
      "   train acc: 0.847951\n",
      "   val loss: 0.762765\n",
      "   val acc: 0.761161\n",
      "Epoch 167 of 200 took 14.054510s\n",
      "   train loss: 0.400672\n",
      "   train acc: 0.839696\n",
      "   val loss: 0.727632\n",
      "   val acc: 0.751116\n",
      "Epoch 168 of 200 took 14.065888s\n",
      "   train loss: 0.403514\n",
      "   train acc: 0.842055\n",
      "   val loss: 0.668457\n",
      "   val acc: 0.755580\n",
      "Epoch 169 of 200 took 14.056639s\n",
      "   train loss: 0.429568\n",
      "   train acc: 0.834021\n",
      "   val loss: 0.736933\n",
      "   val acc: 0.694940\n",
      "Epoch 170 of 200 took 14.051727s\n",
      "   train loss: 0.417167\n",
      "   train acc: 0.837780\n",
      "   val loss: 0.726202\n",
      "   val acc: 0.725074\n",
      "Epoch 171 of 200 took 13.998862s\n",
      "   train loss: 0.398910\n",
      "   train acc: 0.844413\n",
      "   val loss: 0.711809\n",
      "   val acc: 0.744048\n",
      "Epoch 172 of 200 took 14.120229s\n",
      "   train loss: 0.390738\n",
      "   train acc: 0.849278\n",
      "   val loss: 0.720959\n",
      "   val acc: 0.762649\n",
      "Epoch 173 of 200 took 14.046619s\n",
      "   train loss: 0.441489\n",
      "   train acc: 0.832842\n",
      "   val loss: 0.709589\n",
      "   val acc: 0.713914\n",
      "Epoch 174 of 200 took 13.981256s\n",
      "   train loss: 0.477412\n",
      "   train acc: 0.822671\n",
      "   val loss: 0.705171\n",
      "   val acc: 0.694568\n",
      "Epoch 175 of 200 took 14.047076s\n",
      "   train loss: 0.444297\n",
      "   train acc: 0.833063\n",
      "   val loss: 0.699475\n",
      "   val acc: 0.720238\n",
      "Epoch 176 of 200 took 14.227111s\n",
      "   train loss: 0.391797\n",
      "   train acc: 0.848393\n",
      "   val loss: 0.686210\n",
      "   val acc: 0.757440\n",
      "Epoch 177 of 200 took 14.324063s\n",
      "   train loss: 0.405158\n",
      "   train acc: 0.835348\n",
      "   val loss: 0.742571\n",
      "   val acc: 0.761905\n",
      "Epoch 178 of 200 took 14.317158s\n",
      "   train loss: 0.395955\n",
      "   train acc: 0.847435\n",
      "   val loss: 0.695964\n",
      "   val acc: 0.763393\n",
      "Epoch 179 of 200 took 14.364116s\n",
      "   train loss: 0.425111\n",
      "   train acc: 0.831810\n",
      "   val loss: 0.709728\n",
      "   val acc: 0.719122\n",
      "Epoch 180 of 200 took 14.341268s\n",
      "   train loss: 0.403643\n",
      "   train acc: 0.836748\n",
      "   val loss: 0.731492\n",
      "   val acc: 0.765253\n",
      "Epoch 181 of 200 took 14.239927s\n",
      "   train loss: 0.393288\n",
      "   train acc: 0.847435\n",
      "   val loss: 0.700614\n",
      "   val acc: 0.739211\n",
      "Epoch 182 of 200 took 14.250016s\n",
      "   train loss: 0.401388\n",
      "   train acc: 0.842792\n",
      "   val loss: 0.694092\n",
      "   val acc: 0.780506\n",
      "Epoch 183 of 200 took 14.225970s\n",
      "   train loss: 0.397402\n",
      "   train acc: 0.845298\n",
      "   val loss: 0.690685\n",
      "   val acc: 0.778274\n",
      "Epoch 184 of 200 took 14.401377s\n",
      "   train loss: 0.430081\n",
      "   train acc: 0.833874\n",
      "   val loss: 0.645368\n",
      "   val acc: 0.761905\n",
      "Epoch 185 of 200 took 14.353132s\n",
      "   train loss: 0.399585\n",
      "   train acc: 0.842129\n",
      "   val loss: 0.692964\n",
      "   val acc: 0.754464\n",
      "Epoch 186 of 200 took 14.495888s\n",
      "   train loss: 0.412128\n",
      "   train acc: 0.844340\n",
      "   val loss: 0.641299\n",
      "   val acc: 0.767485\n",
      "Epoch 187 of 200 took 14.397309s\n",
      "   train loss: 0.404914\n",
      "   train acc: 0.838886\n",
      "   val loss: 0.786435\n",
      "   val acc: 0.690476\n",
      "Epoch 188 of 200 took 14.376642s\n",
      "   train loss: 0.417515\n",
      "   train acc: 0.839107\n",
      "   val loss: 0.715193\n",
      "   val acc: 0.709821\n",
      "Epoch 189 of 200 took 14.262241s\n",
      "   train loss: 0.413376\n",
      "   train acc: 0.841244\n",
      "   val loss: 0.746233\n",
      "   val acc: 0.775670\n",
      "Epoch 190 of 200 took 14.253950s\n",
      "   train loss: 0.394780\n",
      "   train acc: 0.846772\n",
      "   val loss: 0.716622\n",
      "   val acc: 0.726562\n",
      "Epoch 191 of 200 took 14.363018s\n",
      "   train loss: 0.404212\n",
      "   train acc: 0.839254\n",
      "   val loss: 0.731655\n",
      "   val acc: 0.730655\n",
      "Epoch 192 of 200 took 14.384860s\n",
      "   train loss: 0.386859\n",
      "   train acc: 0.844045\n",
      "   val loss: 0.742536\n",
      "   val acc: 0.756324\n",
      "Epoch 193 of 200 took 14.429955s\n",
      "   train loss: 0.391843\n",
      "   train acc: 0.844929\n",
      "   val loss: 0.690895\n",
      "   val acc: 0.763393\n",
      "Epoch 194 of 200 took 14.326189s\n",
      "   train loss: 0.408196\n",
      "   train acc: 0.838296\n",
      "   val loss: 0.757882\n",
      "   val acc: 0.770461\n",
      "Epoch 195 of 200 took 14.397201s\n",
      "   train loss: 0.423566\n",
      "   train acc: 0.836601\n",
      "   val loss: 0.637730\n",
      "   val acc: 0.767113\n",
      "Epoch 196 of 200 took 14.437730s\n",
      "   train loss: 0.398709\n",
      "   train acc: 0.842866\n",
      "   val loss: 0.723470\n",
      "   val acc: 0.739955\n",
      "Epoch 197 of 200 took 14.488163s\n",
      "   train loss: 0.420569\n",
      "   train acc: 0.839475\n",
      "   val loss: 0.712623\n",
      "   val acc: 0.713542\n",
      "Epoch 198 of 200 took 14.348042s\n",
      "   train loss: 0.408440\n",
      "   train acc: 0.842866\n",
      "   val loss: 0.673654\n",
      "   val acc: 0.741443\n",
      "Epoch 199 of 200 took 14.352312s\n",
      "   train loss: 0.390829\n",
      "   train acc: 0.849351\n",
      "   val loss: 0.707845\n",
      "   val acc: 0.767113\n",
      "Epoch 200 of 200 took 14.252004s\n",
      "   train loss: 0.407075\n",
      "   train acc: 0.839328\n",
      "   val loss: 0.664865\n",
      "   val acc: 0.752232\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "n_epoch = 200\n",
    "print_freq = 1\n",
    "\n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        sess.run(train_op, feed_dict={x: X_train_a, y_: y_train_a})\n",
    "\n",
    "    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
    "        print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
    "        train_loss, train_acc, n_batch = 0, 0, 0\n",
    "        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_train_a, y_: y_train_a})\n",
    "            train_loss += err\n",
    "            train_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "        print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    "        val_loss, val_acc, n_batch = 0, 0, 0\n",
    "        for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n",
    "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_val_a, y_: y_val_a})\n",
    "            val_loss += err\n",
    "            val_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   val loss: %f\" % (val_loss / n_batch))\n",
    "        print(\"   val acc: %f\" % (val_acc / n_batch))\n",
    "        \n",
    "winsound.PlaySound(\"*\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test loss: 0.662489\n",
      "   test acc: 0.700810\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, n_batch = 0, 0, 0\n",
    "for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n",
    "    err, ac = sess.run([cost_test, acc], feed_dict={x: X_test_a, y_: y_test_a})\n",
    "    test_loss += err\n",
    "    test_acc += ac\n",
    "    n_batch += 1\n",
    "print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "print(\"   test acc: %f\" % (test_acc / n_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] [*] model2d.npz saved\n"
     ]
    }
   ],
   "source": [
    "tl.files.save_npz(net_train.all_params, name='model2d.npz')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] [*] Load model2d.npz SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorlayer.layers.core.DenseLayer at 0x19a443c9400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.files.load_and_assign_npz(sess=sess, name='model2d.npz', network=net_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling TensorRider Using the Generated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython import display\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import socket\n",
    "import threading\n",
    "from time import ctime,sleep\n",
    "import string\n",
    "\n",
    "remoteImage = np.array([])\n",
    "stream = urllib.request.urlopen('http://192.168.73.73:8080/?action=stream&ignored.mjpg')\n",
    "bytes = bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_Refreshing_Thread():\n",
    "    global remoteImage\n",
    "    global stream\n",
    "    global bytes\n",
    "    while True:\n",
    "        bytes += stream.read(1024)\n",
    "        a = bytes.find(b'\\xff\\xd8')\n",
    "        b = bytes.find(b'\\xff\\xd9')\n",
    "        if a != -1 and b != -1:\n",
    "            jpg = bytes[a:b+2]\n",
    "            bytes = bytes[b+2:]\n",
    "            i = cv2.imdecode(np.fromstring(jpg, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "            i = rgb2mono(i)\n",
    "            remoteImage = i.reshape((1,60,80,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Controlling_Thread():\n",
    "    global remoteImage\n",
    "    imgArray4d = np.zeros((128,60,80,1))\n",
    "    addr=('192.168.73.73',51423)\n",
    "    s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n",
    "    while True:\n",
    "        for index in range(128):\n",
    "            imgArray4d[index,:,:,0] = remoteImage[0,:,:,0]\n",
    "            \n",
    "        direction = tl.utils.predict(sess, net_test, imgArray4d, x, y_op, batch_size=None)\n",
    "        msgCtrl_Udp = str(25) + \",\" + str((direction[0] - 3) * 15)\n",
    "#         print(msgCtrl_Udp)\n",
    "        s.sendto(msgCtrl_Udp.encode('utf-8'), addr)\n",
    "        sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_op = tf.argmax(tf.nn.softmax(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "RefreshImageThread = threading.Thread(target = Image_Refreshing_Thread)\n",
    "RefreshImageThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "ControllingThread = threading.Thread(target = Controlling_Thread)\n",
    "ControllingThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
