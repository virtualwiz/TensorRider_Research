{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRider Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import time\n",
    "\n",
    "from tensorlayer.layers import *\n",
    "LayersConfig.tf_dtype = tf.float16  # tf.float32  tf.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2mono(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    # mono = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    mono = 0.5 * r + 0.25 * g + 0.25 * b\n",
    "    return mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataArrays(iterator):\n",
    "    X = np.zeros(4800)\n",
    "    y = np.zeros(1)\n",
    "    recordCounter = 0;\n",
    "\n",
    "    for string_record in iterator:\n",
    "        recordCounter += 1\n",
    "\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(string_record)\n",
    "        imageString = (example.features.feature['image'].bytes_list.value[0])\n",
    "        label = (example.features.feature['label'].int64_list.value[0])\n",
    "\n",
    "        image = np.fromstring(imageString, dtype=np.uint8)\n",
    "        image = image.reshape((60, 80, 3))\n",
    "        image = rgb2mono(image)\n",
    "        image = image.reshape((4800))\n",
    "        \n",
    "        X = np.append(X,image)\n",
    "        y = np.append(y,label)\n",
    "        if recordCounter % 100 == 0:\n",
    "            print(recordCounter,end = '->')\n",
    "            \n",
    "    X = X.reshape((recordCounter + 1, 60, 80, 1))\n",
    "    y = y.reshape((recordCounter + 1,))\n",
    "    y = np.round(y / 12)\n",
    "    y = y + 3\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIterator = tf.python_io.tf_record_iterator(path=\"train.tfrecords\")\n",
    "valIterator = tf.python_io.tf_record_iterator(path=\"val.tfrecords\")\n",
    "testIterator = tf.python_io.tf_record_iterator(path=\"test.tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecords -> Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train...\n",
      "100->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200->300->400->500->600->700->800->900->1000->1100->1200->1300->1400->1500->1600->1700->1800->1900->2000->2100->2200->2300->2400->2500->2600->2700->2800->2900->3000->3100->3200->3300->3400->3500->3600->3700->3800->3900->4000->4100->4200->4300->4400->4500->4600->4700->4800->4900->5000->5100->5200->5300->5400->5500->5600->5700->5800->5900->6000->6100->6200->6300->6400->6500->6600->6700->6800->6900->7000->7100->7200->7300->7400->7500->7600->7700->7800->7900->8000->8100->8200->8300->8400->8500->8600->8700->8800->8900->9000->9100->9200->9300->9400->9500->9600->9700->9800->9900->10000->10100->10200->10300->10400->10500->10600->10700->10800->10900->11000->11100->11200->11300->11400->11500->11600->11700->11800->11900->12000->12100->12200->12300->12400->12500->12600->12700->12800->12900->13000->13100->13200->13300->13400->13500->\n",
      "Val...\n",
      "100->200->300->400->500->600->700->800->900->1000->1100->1200->1300->1400->1500->1600->1700->1800->1900->2000->2100->2200->2300->2400->2500->2600->2700->\n",
      "Test...\n",
      "100->200->300->400->500->600->700->800->900->1000->1100->1200->1300->1400->1500->1600->1700->1800->1900->2000->2100->2200->2300->2400->2500->2600->2700->2800->2900->3000->3100->3200->3300->3400->3500->"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain...\")\n",
    "X_train, y_train = prepareDataArrays(trainIterator)\n",
    "print(\"\\nVal...\")\n",
    "X_val, y_val = prepareDataArrays(valIterator)\n",
    "print(\"\\nTest...\")\n",
    "X_test, y_test = prepareDataArrays(testIterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays -> NPZ File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"arrays2d.npz\", xtr = X_train, ytr = y_train, xval = X_val, yval = y_val, xt = X_test, yt = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPZ File -> Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "npRecall = np.load(\"arrays2d.npz\")\n",
    "X_train = npRecall[\"xtr\"]\n",
    "y_train = npRecall[\"ytr\"]\n",
    "X_val = npRecall[\"xval\"]\n",
    "y_val = npRecall[\"yval\"]\n",
    "X_test = npRecall[\"xt\"]\n",
    "y_test = npRecall[\"yt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.930e+02, 0.000e+00, 4.565e+03, 0.000e+00, 4.786e+03, 0.000e+00,\n",
       "        3.352e+03, 0.000e+00, 3.960e+02, 3.000e+00]),\n",
       " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train)\n",
    "# plt.hist(y_val)\n",
    "# plt.hist(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "x = tf.placeholder(LayersConfig.tf_dtype, shape=[batch_size, 60, 80, 1])\n",
    "y_ = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "\n",
    "# x = tf.placeholder(tf.float32, shape=[None, 4800], name='x')\n",
    "# y_ = tf.placeholder(tf.int64, shape=[None], name='y_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        n = InputLayer(x, name='input')\n",
    "        # cnn\n",
    "        n = Conv2d(n, 32, (5, 5), (1, 1), padding='SAME', name='cnn1')\n",
    "        n = BatchNormLayer(n, act=tf.nn.relu, is_train=is_train, name='bn1')\n",
    "        n = MaxPool2d(n, (2, 2), (2, 2), padding='SAME', name='pool1')\n",
    "        n = Conv2d(n, 64, (5, 5), (1, 1), padding='SAME', name='cnn2')\n",
    "        n = BatchNormLayer(n, act=tf.nn.relu, is_train=is_train, name='bn2')\n",
    "        n = MaxPool2d(n, (2, 2), (2, 2), padding='SAME', name='pool2')\n",
    "        # mlp\n",
    "        n = FlattenLayer(n, name='flatten')\n",
    "        n = DropoutLayer(n, 0.5, True, is_train, name='drop1')\n",
    "        n = DenseLayer(n, 256, act=tf.nn.relu, name='relu1')\n",
    "        n = DropoutLayer(n, 0.5, True, is_train, name='drop2')\n",
    "        n = DenseLayer(n, 7, act=tf.identity, name='output')\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  model/input: (128, 60, 80, 1)\n",
      "[TL] Conv2dLayer model/cnn1: shape:(5, 5, 1, 32) strides:(1, 1, 1, 1) pad:SAME act:identity\n",
      "[TL] BatchNormLayer model/bn1: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "[TL] PoolLayer   model/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "[TL] Conv2dLayer model/cnn2: shape:(5, 5, 32, 64) strides:(1, 1, 1, 1) pad:SAME act:identity\n",
      "[TL] BatchNormLayer model/bn2: decay:0.900000 epsilon:0.000010 act:relu is_train:True\n",
      "[TL] PoolLayer   model/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "[TL] FlattenLayer model/flatten: 19200\n",
      "[TL] DropoutLayer model/drop1: keep:0.500000 is_fix:True\n",
      "[TL] DenseLayer  model/relu1: 256 relu\n",
      "[TL] DropoutLayer model/drop2: keep:0.500000 is_fix:True\n",
      "[TL] DenseLayer  model/output: 7 identity\n",
      "[TL] InputLayer  model/input: (128, 60, 80, 1)\n",
      "[TL] Conv2dLayer model/cnn1: shape:(5, 5, 1, 32) strides:(1, 1, 1, 1) pad:SAME act:identity\n",
      "[TL] BatchNormLayer model/bn1: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "[TL] PoolLayer   model/pool1: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "[TL] Conv2dLayer model/cnn2: shape:(5, 5, 32, 64) strides:(1, 1, 1, 1) pad:SAME act:identity\n",
      "[TL] BatchNormLayer model/bn2: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "[TL] PoolLayer   model/pool2: ksize:[1, 2, 2, 1] strides:[1, 2, 2, 1] padding:SAME pool:max_pool\n",
      "[TL] FlattenLayer model/flatten: 19200\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  model/relu1: 256 relu\n",
      "[TL]   skip DropoutLayer\n",
      "[TL] DenseLayer  model/output: 7 identity\n",
      "[TL]   param   0: model/cnn1/W_conv2d:0 (5, 5, 1, 32)      float16_ref\n",
      "[TL]   param   1: model/cnn1/b_conv2d:0 (32,)              float16_ref\n",
      "[TL]   param   2: model/bn1/beta:0     (32,)              float16_ref\n",
      "[TL]   param   3: model/bn1/gamma:0    (32,)              float16_ref\n",
      "[TL]   param   4: model/bn1/moving_mean:0 (32,)              float16_ref\n",
      "[TL]   param   5: model/bn1/moving_variance:0 (32,)              float16_ref\n",
      "[TL]   param   6: model/cnn2/W_conv2d:0 (5, 5, 32, 64)     float16_ref\n",
      "[TL]   param   7: model/cnn2/b_conv2d:0 (64,)              float16_ref\n",
      "[TL]   param   8: model/bn2/beta:0     (64,)              float16_ref\n",
      "[TL]   param   9: model/bn2/gamma:0    (64,)              float16_ref\n",
      "[TL]   param  10: model/bn2/moving_mean:0 (64,)              float16_ref\n",
      "[TL]   param  11: model/bn2/moving_variance:0 (64,)              float16_ref\n",
      "[TL]   param  12: model/relu1/W:0      (19200, 256)       float16_ref\n",
      "[TL]   param  13: model/relu1/b:0      (256,)             float16_ref\n",
      "[TL]   param  14: model/output/W:0     (256, 7)           float16_ref\n",
      "[TL]   param  15: model/output/b:0     (7,)               float16_ref\n",
      "[TL]   num of params: 4969735\n"
     ]
    }
   ],
   "source": [
    "# define inferences\n",
    "net_train = model(x, is_train=True, reuse=False)\n",
    "net_test = model(x, is_train=False, reuse=True)\n",
    "\n",
    "net_train.print_params(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost for training\n",
    "y = net_train.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "\n",
    "# cost and accuracy for evalution\n",
    "y2 = net_test.outputs\n",
    "cost_test = tl.cost.cross_entropy(y2, y_, name='xentropy2')\n",
    "correct_prediction = tf.equal(tf.argmax(y2, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, LayersConfig.tf_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   [*] geting variables with model\n"
     ]
    }
   ],
   "source": [
    "# define the optimizer\n",
    "train_params = tl.layers.get_variables_with_name('model', train_only=True, printable=False)\n",
    "train_op = tf.train.AdamOptimizer(\n",
    "    learning_rate=0.0001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    # epsilon=1e-08,    # for float32 as default\n",
    "    epsilon=1e-4,  # for float16, see https://stackoverflow.com/questions/42064941/tensorflow-float16-support-is-broken\n",
    "    use_locking=False).minimize(cost, var_list=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all variables in the session\n",
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Setting up TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_summ = tf.summary.scalar('acc', acc)  \n",
    "cost_summ = tf.summary.scalar('cost', cost)  \n",
    "summary = tf.summary.merge_all()  \n",
    "writer = tf.summary.FileWriter('./logs')  \n",
    "writer.add_graph(sess.graph) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 200 took 15.231647s\n",
      "   train loss: 1.222454\n",
      "   train acc: 0.694060\n",
      "   val loss: 1.348563\n",
      "   val acc: 0.686012\n",
      "Epoch 2 of 200 took 13.773329s\n",
      "   train loss: 0.765924\n",
      "   train acc: 0.724867\n",
      "   val loss: 0.870257\n",
      "   val acc: 0.715402\n",
      "Epoch 3 of 200 took 13.955816s\n",
      "   train loss: 0.761571\n",
      "   train acc: 0.741893\n",
      "   val loss: 0.902762\n",
      "   val acc: 0.700893\n",
      "Epoch 4 of 200 took 14.043663s\n",
      "   train loss: 0.741925\n",
      "   train acc: 0.745946\n",
      "   val loss: 0.885138\n",
      "   val acc: 0.707217\n",
      "Epoch 5 of 200 took 14.139515s\n",
      "   train loss: 0.739543\n",
      "   train acc: 0.752211\n",
      "   val loss: 0.855934\n",
      "   val acc: 0.728795\n",
      "Epoch 6 of 200 took 14.344675s\n",
      "   train loss: 0.685528\n",
      "   train acc: 0.756486\n",
      "   val loss: 0.793783\n",
      "   val acc: 0.744048\n",
      "Epoch 7 of 200 took 14.350270s\n",
      "   train loss: 0.666771\n",
      "   train acc: 0.758476\n",
      "   val loss: 0.784180\n",
      "   val acc: 0.739955\n",
      "Epoch 8 of 200 took 14.396872s\n",
      "   train loss: 0.640438\n",
      "   train acc: 0.755012\n",
      "   val loss: 0.755673\n",
      "   val acc: 0.741815\n",
      "Epoch 9 of 200 took 14.489808s\n",
      "   train loss: 0.657001\n",
      "   train acc: 0.758771\n",
      "   val loss: 0.775228\n",
      "   val acc: 0.733259\n",
      "Epoch 10 of 200 took 14.579915s\n",
      "   train loss: 0.664219\n",
      "   train acc: 0.758402\n",
      "   val loss: 0.775902\n",
      "   val acc: 0.731027\n",
      "Epoch 11 of 200 took 14.479923s\n",
      "   train loss: 0.644025\n",
      "   train acc: 0.756633\n",
      "   val loss: 0.758626\n",
      "   val acc: 0.742188\n",
      "Epoch 12 of 200 took 14.539098s\n",
      "   train loss: 0.638064\n",
      "   train acc: 0.759950\n",
      "   val loss: 0.763253\n",
      "   val acc: 0.742188\n",
      "Epoch 13 of 200 took 14.540589s\n",
      "   train loss: 0.629984\n",
      "   train acc: 0.759581\n",
      "   val loss: 0.761416\n",
      "   val acc: 0.741071\n",
      "Epoch 14 of 200 took 14.186954s\n",
      "   train loss: 0.640828\n",
      "   train acc: 0.762308\n",
      "   val loss: 0.762254\n",
      "   val acc: 0.738839\n",
      "Epoch 15 of 200 took 14.150864s\n",
      "   train loss: 0.627172\n",
      "   train acc: 0.761129\n",
      "   val loss: 0.760161\n",
      "   val acc: 0.741815\n",
      "Epoch 16 of 200 took 14.235425s\n",
      "   train loss: 0.636848\n",
      "   train acc: 0.761719\n",
      "   val loss: 0.758789\n",
      "   val acc: 0.741443\n",
      "Epoch 17 of 200 took 14.246881s\n",
      "   train loss: 0.626792\n",
      "   train acc: 0.761792\n",
      "   val loss: 0.757533\n",
      "   val acc: 0.745908\n",
      "Epoch 18 of 200 took 14.252550s\n",
      "   train loss: 0.621727\n",
      "   train acc: 0.762603\n",
      "   val loss: 0.751093\n",
      "   val acc: 0.746652\n",
      "Epoch 19 of 200 took 14.409719s\n",
      "   train loss: 0.618309\n",
      "   train acc: 0.761866\n",
      "   val loss: 0.748024\n",
      "   val acc: 0.747768\n",
      "Epoch 20 of 200 took 14.419858s\n",
      "   train loss: 0.617740\n",
      "   train acc: 0.762382\n",
      "   val loss: 0.753069\n",
      "   val acc: 0.741815\n",
      "Epoch 21 of 200 took 14.473603s\n",
      "   train loss: 0.614571\n",
      "   train acc: 0.763561\n",
      "   val loss: 0.747861\n",
      "   val acc: 0.747768\n",
      "Epoch 22 of 200 took 14.385821s\n",
      "   train loss: 0.617584\n",
      "   train acc: 0.761277\n",
      "   val loss: 0.757533\n",
      "   val acc: 0.743304\n",
      "Epoch 23 of 200 took 14.642442s\n",
      "   train loss: 0.609969\n",
      "   train acc: 0.764446\n",
      "   val loss: 0.747559\n",
      "   val acc: 0.741815\n",
      "Epoch 24 of 200 took 14.625430s\n",
      "   train loss: 0.611876\n",
      "   train acc: 0.765256\n",
      "   val loss: 0.754302\n",
      "   val acc: 0.748140\n",
      "Epoch 25 of 200 took 14.560387s\n",
      "   train loss: 0.606761\n",
      "   train acc: 0.763856\n",
      "   val loss: 0.745698\n",
      "   val acc: 0.746280\n",
      "Epoch 26 of 200 took 14.584038s\n",
      "   train loss: 0.612466\n",
      "   train acc: 0.765035\n",
      "   val loss: 0.758092\n",
      "   val acc: 0.736607\n",
      "Epoch 27 of 200 took 14.607053s\n",
      "   train loss: 0.611006\n",
      "   train acc: 0.766141\n",
      "   val loss: 0.748744\n",
      "   val acc: 0.743304\n",
      "Epoch 28 of 200 took 14.603113s\n",
      "   train loss: 0.612475\n",
      "   train acc: 0.768205\n",
      "   val loss: 0.749837\n",
      "   val acc: 0.742560\n",
      "Epoch 29 of 200 took 14.368327s\n",
      "   train loss: 0.610715\n",
      "   train acc: 0.768794\n",
      "   val loss: 0.748930\n",
      "   val acc: 0.744048\n",
      "Epoch 30 of 200 took 14.424016s\n",
      "   train loss: 0.604775\n",
      "   train acc: 0.764667\n",
      "   val loss: 0.767671\n",
      "   val acc: 0.743304\n",
      "Epoch 31 of 200 took 14.391959s\n",
      "   train loss: 0.602988\n",
      "   train acc: 0.765994\n",
      "   val loss: 0.754883\n",
      "   val acc: 0.741443\n",
      "Epoch 32 of 200 took 14.383126s\n",
      "   train loss: 0.597177\n",
      "   train acc: 0.768647\n",
      "   val loss: 0.755208\n",
      "   val acc: 0.744048\n",
      "Epoch 33 of 200 took 14.437423s\n",
      "   train loss: 0.616273\n",
      "   train acc: 0.767099\n",
      "   val loss: 0.754139\n",
      "   val acc: 0.736235\n",
      "Epoch 34 of 200 took 14.273473s\n",
      "   train loss: 0.603221\n",
      "   train acc: 0.769236\n",
      "   val loss: 0.752906\n",
      "   val acc: 0.737723\n",
      "Epoch 35 of 200 took 14.348466s\n",
      "   train loss: 0.589215\n",
      "   train acc: 0.770563\n",
      "   val loss: 0.745047\n",
      "   val acc: 0.742188\n",
      "Epoch 36 of 200 took 14.407914s\n",
      "   train loss: 0.584998\n",
      "   train acc: 0.771300\n",
      "   val loss: 0.745396\n",
      "   val acc: 0.741815\n",
      "Epoch 37 of 200 took 14.313855s\n",
      "   train loss: 0.588003\n",
      "   train acc: 0.775796\n",
      "   val loss: 0.738444\n",
      "   val acc: 0.744420\n",
      "Epoch 38 of 200 took 14.283281s\n",
      "   train loss: 0.576626\n",
      "   train acc: 0.773216\n",
      "   val loss: 0.753604\n",
      "   val acc: 0.744420\n",
      "Epoch 39 of 200 took 14.328241s\n",
      "   train loss: 0.575714\n",
      "   train acc: 0.774027\n",
      "   val loss: 0.752767\n",
      "   val acc: 0.740699\n",
      "Epoch 40 of 200 took 14.253106s\n",
      "   train loss: 0.589088\n",
      "   train acc: 0.772995\n",
      "   val loss: 0.750047\n",
      "   val acc: 0.735491\n",
      "Epoch 41 of 200 took 14.237373s\n",
      "   train loss: 0.595558\n",
      "   train acc: 0.773438\n",
      "   val loss: 0.749535\n",
      "   val acc: 0.732143\n",
      "Epoch 42 of 200 took 14.429127s\n",
      "   train loss: 0.578751\n",
      "   train acc: 0.776459\n",
      "   val loss: 0.741443\n",
      "   val acc: 0.743304\n",
      "Epoch 43 of 200 took 14.412435s\n",
      "   train loss: 0.570980\n",
      "   train acc: 0.776680\n",
      "   val loss: 0.732445\n",
      "   val acc: 0.747768\n",
      "Epoch 44 of 200 took 14.508063s\n",
      "   train loss: 0.563847\n",
      "   train acc: 0.778007\n",
      "   val loss: 0.739304\n",
      "   val acc: 0.745164\n",
      "Epoch 45 of 200 took 14.511537s\n",
      "   train loss: 0.567288\n",
      "   train acc: 0.777196\n",
      "   val loss: 0.745164\n",
      "   val acc: 0.739211\n",
      "Epoch 46 of 200 took 14.383651s\n",
      "   train loss: 0.567901\n",
      "   train acc: 0.778449\n",
      "   val loss: 0.738491\n",
      "   val acc: 0.746652\n",
      "Epoch 47 of 200 took 14.348315s\n",
      "   train loss: 0.552903\n",
      "   train acc: 0.783535\n",
      "   val loss: 0.727958\n",
      "   val acc: 0.745908\n",
      "Epoch 48 of 200 took 14.431926s\n",
      "   train loss: 0.547541\n",
      "   train acc: 0.785009\n",
      "   val loss: 0.729143\n",
      "   val acc: 0.746280\n",
      "Epoch 49 of 200 took 14.576145s\n",
      "   train loss: 0.573505\n",
      "   train acc: 0.783535\n",
      "   val loss: 0.726981\n",
      "   val acc: 0.741815\n",
      "Epoch 50 of 200 took 14.418381s\n",
      "   train loss: 0.537757\n",
      "   train acc: 0.785893\n",
      "   val loss: 0.736142\n",
      "   val acc: 0.751488\n",
      "Epoch 51 of 200 took 14.366802s\n",
      "   train loss: 0.552509\n",
      "   train acc: 0.782871\n",
      "   val loss: 0.750721\n",
      "   val acc: 0.754092\n",
      "Epoch 52 of 200 took 14.410793s\n",
      "   train loss: 0.535951\n",
      "   train acc: 0.788694\n",
      "   val loss: 0.715355\n",
      "   val acc: 0.752232\n",
      "Epoch 53 of 200 took 14.492334s\n",
      "   train loss: 0.527443\n",
      "   train acc: 0.790684\n",
      "   val loss: 0.733305\n",
      "   val acc: 0.753348\n",
      "Epoch 54 of 200 took 14.295752s\n",
      "   train loss: 0.529228\n",
      "   train acc: 0.788547\n",
      "   val loss: 0.718355\n",
      "   val acc: 0.749628\n",
      "Epoch 55 of 200 took 14.274140s\n",
      "   train loss: 0.540152\n",
      "   train acc: 0.793264\n",
      "   val loss: 0.730911\n",
      "   val acc: 0.752604\n",
      "Epoch 56 of 200 took 14.270532s\n",
      "   train loss: 0.518311\n",
      "   train acc: 0.792527\n",
      "   val loss: 0.719308\n",
      "   val acc: 0.752976\n",
      "Epoch 57 of 200 took 14.359792s\n",
      "   train loss: 0.515369\n",
      "   train acc: 0.793706\n",
      "   val loss: 0.723028\n",
      "   val acc: 0.755208\n",
      "Epoch 58 of 200 took 14.424911s\n",
      "   train loss: 0.523940\n",
      "   train acc: 0.795401\n",
      "   val loss: 0.719994\n",
      "   val acc: 0.755580\n",
      "Epoch 59 of 200 took 14.513500s\n",
      "   train loss: 0.495935\n",
      "   train acc: 0.799307\n",
      "   val loss: 0.723389\n",
      "   val acc: 0.756696\n",
      "Epoch 60 of 200 took 14.611723s\n",
      "   train loss: 0.498945\n",
      "   train acc: 0.803950\n",
      "   val loss: 0.711821\n",
      "   val acc: 0.754464\n",
      "Epoch 61 of 200 took 14.434827s\n",
      "   train loss: 0.503178\n",
      "   train acc: 0.801960\n",
      "   val loss: 0.733224\n",
      "   val acc: 0.754836\n",
      "Epoch 62 of 200 took 14.166762s\n",
      "   train loss: 0.487904\n",
      "   train acc: 0.804024\n",
      "   val loss: 0.714623\n",
      "   val acc: 0.756324\n",
      "Epoch 63 of 200 took 14.309767s\n",
      "   train loss: 0.491377\n",
      "   train acc: 0.803582\n",
      "   val loss: 0.696894\n",
      "   val acc: 0.754836\n",
      "Epoch 64 of 200 took 14.386296s\n",
      "   train loss: 0.484872\n",
      "   train acc: 0.804319\n",
      "   val loss: 0.705659\n",
      "   val acc: 0.754464\n",
      "Epoch 65 of 200 took 14.516909s\n",
      "   train loss: 0.475224\n",
      "   train acc: 0.808815\n",
      "   val loss: 0.693824\n",
      "   val acc: 0.757812\n",
      "Epoch 66 of 200 took 14.466185s\n",
      "   train loss: 0.493625\n",
      "   train acc: 0.808815\n",
      "   val loss: 0.682059\n",
      "   val acc: 0.752976\n",
      "Epoch 67 of 200 took 14.605527s\n",
      "   train loss: 0.470178\n",
      "   train acc: 0.811616\n",
      "   val loss: 0.698172\n",
      "   val acc: 0.752604\n",
      "Epoch 68 of 200 took 14.411564s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train loss: 0.499843\n",
      "   train acc: 0.809404\n",
      "   val loss: 0.668969\n",
      "   val acc: 0.762277\n",
      "Epoch 69 of 200 took 14.326508s\n",
      "   train loss: 0.471320\n",
      "   train acc: 0.813679\n",
      "   val loss: 0.696673\n",
      "   val acc: 0.761533\n",
      "Epoch 70 of 200 took 14.267322s\n",
      "   train loss: 0.472753\n",
      "   train acc: 0.816848\n",
      "   val loss: 0.698172\n",
      "   val acc: 0.745536\n",
      "Epoch 71 of 200 took 14.407527s\n",
      "   train loss: 0.464413\n",
      "   train acc: 0.815301\n",
      "   val loss: 0.691441\n",
      "   val acc: 0.753720\n",
      "Epoch 72 of 200 took 14.429024s\n",
      "   train loss: 0.471369\n",
      "   train acc: 0.812205\n",
      "   val loss: 0.698963\n",
      "   val acc: 0.757812\n",
      "Epoch 73 of 200 took 14.509150s\n",
      "   train loss: 0.470910\n",
      "   train acc: 0.814343\n",
      "   val loss: 0.650658\n",
      "   val acc: 0.760045\n",
      "Epoch 74 of 200 took 14.378562s\n",
      "   train loss: 0.462941\n",
      "   train acc: 0.815522\n",
      "   val loss: 0.701881\n",
      "   val acc: 0.758557\n",
      "Epoch 75 of 200 took 14.384334s\n",
      "   train loss: 0.463934\n",
      "   train acc: 0.820902\n",
      "   val loss: 0.713309\n",
      "   val acc: 0.739955\n",
      "Epoch 76 of 200 took 14.517349s\n",
      "   train loss: 0.463563\n",
      "   train acc: 0.818765\n",
      "   val loss: 0.680873\n",
      "   val acc: 0.752976\n",
      "Epoch 77 of 200 took 14.384478s\n",
      "   train loss: 0.464908\n",
      "   train acc: 0.819575\n",
      "   val loss: 0.676374\n",
      "   val acc: 0.758929\n",
      "Epoch 78 of 200 took 14.487960s\n",
      "   train loss: 0.469558\n",
      "   train acc: 0.816554\n",
      "   val loss: 0.659761\n",
      "   val acc: 0.764137\n",
      "Epoch 79 of 200 took 14.406575s\n",
      "   train loss: 0.450907\n",
      "   train acc: 0.820976\n",
      "   val loss: 0.682710\n",
      "   val acc: 0.762649\n",
      "Epoch 80 of 200 took 14.543585s\n",
      "   train loss: 0.449329\n",
      "   train acc: 0.824219\n",
      "   val loss: 0.695103\n",
      "   val acc: 0.752604\n",
      "Epoch 81 of 200 took 14.315725s\n",
      "   train loss: 0.460997\n",
      "   train acc: 0.825103\n",
      "   val loss: 0.692755\n",
      "   val acc: 0.745164\n",
      "Epoch 82 of 200 took 14.386112s\n",
      "   train loss: 0.466417\n",
      "   train acc: 0.817217\n",
      "   val loss: 0.648879\n",
      "   val acc: 0.763393\n",
      "Epoch 83 of 200 took 14.222492s\n",
      "   train loss: 0.443562\n",
      "   train acc: 0.825840\n",
      "   val loss: 0.698626\n",
      "   val acc: 0.755208\n",
      "Epoch 84 of 200 took 14.405445s\n",
      "   train loss: 0.454521\n",
      "   train acc: 0.823629\n",
      "   val loss: 0.705171\n",
      "   val acc: 0.750744\n",
      "Epoch 85 of 200 took 14.260160s\n",
      "   train loss: 0.443981\n",
      "   train acc: 0.826504\n",
      "   val loss: 0.675653\n",
      "   val acc: 0.767857\n",
      "Epoch 86 of 200 took 14.489051s\n",
      "   train loss: 0.451381\n",
      "   train acc: 0.824587\n",
      "   val loss: 0.680780\n",
      "   val acc: 0.746280\n",
      "Epoch 87 of 200 took 14.364607s\n",
      "   train loss: 0.439142\n",
      "   train acc: 0.828862\n",
      "   val loss: 0.665283\n",
      "   val acc: 0.762277\n",
      "Epoch 88 of 200 took 14.370351s\n",
      "   train loss: 0.464498\n",
      "   train acc: 0.821713\n",
      "   val loss: 0.680850\n",
      "   val acc: 0.740327\n",
      "Epoch 89 of 200 took 14.431733s\n",
      "   train loss: 0.448309\n",
      "   train acc: 0.825472\n",
      "   val loss: 0.673921\n",
      "   val acc: 0.747768\n",
      "Epoch 90 of 200 took 14.344604s\n",
      "   train loss: 0.447512\n",
      "   train acc: 0.822818\n",
      "   val loss: 0.717041\n",
      "   val acc: 0.735863\n",
      "Epoch 91 of 200 took 14.395831s\n",
      "   train loss: 0.445960\n",
      "   train acc: 0.824514\n",
      "   val loss: 0.727214\n",
      "   val acc: 0.750372\n",
      "Epoch 92 of 200 took 14.333486s\n",
      "   train loss: 0.443187\n",
      "   train acc: 0.824514\n",
      "   val loss: 0.693545\n",
      "   val acc: 0.735863\n",
      "Epoch 93 of 200 took 14.102053s\n",
      "   train loss: 0.442445\n",
      "   train acc: 0.826946\n",
      "   val loss: 0.669317\n",
      "   val acc: 0.757440\n",
      "Epoch 94 of 200 took 14.160391s\n",
      "   train loss: 0.433695\n",
      "   train acc: 0.830557\n",
      "   val loss: 0.678560\n",
      "   val acc: 0.761905\n",
      "Epoch 95 of 200 took 14.173057s\n",
      "   train loss: 0.449253\n",
      "   train acc: 0.826872\n",
      "   val loss: 0.673619\n",
      "   val acc: 0.750744\n",
      "Epoch 96 of 200 took 14.328383s\n",
      "   train loss: 0.439186\n",
      "   train acc: 0.830557\n",
      "   val loss: 0.675316\n",
      "   val acc: 0.747768\n",
      "Epoch 97 of 200 took 14.397810s\n",
      "   train loss: 0.433465\n",
      "   train acc: 0.829820\n",
      "   val loss: 0.705927\n",
      "   val acc: 0.753720\n",
      "Epoch 98 of 200 took 14.155298s\n",
      "   train loss: 0.439951\n",
      "   train acc: 0.828641\n",
      "   val loss: 0.677734\n",
      "   val acc: 0.749256\n",
      "Epoch 99 of 200 took 14.167831s\n",
      "   train loss: 0.442922\n",
      "   train acc: 0.830336\n",
      "   val loss: 0.699195\n",
      "   val acc: 0.739583\n",
      "Epoch 100 of 200 took 14.302139s\n",
      "   train loss: 0.433320\n",
      "   train acc: 0.829157\n",
      "   val loss: 0.670422\n",
      "   val acc: 0.758929\n",
      "Epoch 101 of 200 took 14.403656s\n",
      "   train loss: 0.431514\n",
      "   train acc: 0.831442\n",
      "   val loss: 0.687326\n",
      "   val acc: 0.749628\n",
      "Epoch 102 of 200 took 14.527977s\n",
      "   train loss: 0.443051\n",
      "   train acc: 0.829157\n",
      "   val loss: 0.684129\n",
      "   val acc: 0.746280\n",
      "Epoch 103 of 200 took 14.416400s\n",
      "   train loss: 0.444578\n",
      "   train acc: 0.830115\n",
      "   val loss: 0.660273\n",
      "   val acc: 0.759301\n",
      "Epoch 104 of 200 took 14.394878s\n",
      "   train loss: 0.444884\n",
      "   train acc: 0.829820\n",
      "   val loss: 0.663714\n",
      "   val acc: 0.748512\n",
      "Epoch 105 of 200 took 14.542449s\n",
      "   train loss: 0.448521\n",
      "   train acc: 0.833505\n",
      "   val loss: 0.664586\n",
      "   val acc: 0.762649\n",
      "Epoch 106 of 200 took 14.250615s\n",
      "   train loss: 0.438530\n",
      "   train acc: 0.828862\n",
      "   val loss: 0.667759\n",
      "   val acc: 0.755208\n",
      "Epoch 107 of 200 took 14.159532s\n",
      "   train loss: 0.430565\n",
      "   train acc: 0.829968\n",
      "   val loss: 0.671108\n",
      "   val acc: 0.753348\n",
      "Epoch 108 of 200 took 14.230418s\n",
      "   train loss: 0.440163\n",
      "   train acc: 0.830262\n",
      "   val loss: 0.665213\n",
      "   val acc: 0.757812\n",
      "Epoch 109 of 200 took 14.135094s\n",
      "   train loss: 0.429842\n",
      "   train acc: 0.826872\n",
      "   val loss: 0.743815\n",
      "   val acc: 0.731771\n",
      "Epoch 110 of 200 took 14.176133s\n",
      "   train loss: 0.432795\n",
      "   train acc: 0.830262\n",
      "   val loss: 0.682978\n",
      "   val acc: 0.764509\n",
      "Epoch 111 of 200 took 14.281774s\n",
      "   train loss: 0.442159\n",
      "   train acc: 0.824440\n",
      "   val loss: 0.700684\n",
      "   val acc: 0.732887\n",
      "Epoch 112 of 200 took 14.253955s\n",
      "   train loss: 0.451243\n",
      "   train acc: 0.830852\n",
      "   val loss: 0.662691\n",
      "   val acc: 0.746652\n",
      "Epoch 113 of 200 took 14.297391s\n",
      "   train loss: 0.422985\n",
      "   train acc: 0.832031\n",
      "   val loss: 0.681734\n",
      "   val acc: 0.755580\n",
      "Epoch 114 of 200 took 14.378555s\n",
      "   train loss: 0.430192\n",
      "   train acc: 0.831736\n",
      "   val loss: 0.675758\n",
      "   val acc: 0.757440\n",
      "Epoch 115 of 200 took 14.130572s\n",
      "   train loss: 0.428393\n",
      "   train acc: 0.830778\n",
      "   val loss: 0.712681\n",
      "   val acc: 0.745536\n",
      "Epoch 116 of 200 took 14.188194s\n",
      "   train loss: 0.423842\n",
      "   train acc: 0.832179\n",
      "   val loss: 0.688895\n",
      "   val acc: 0.757068\n",
      "Epoch 117 of 200 took 14.342595s\n",
      "   train loss: 0.426926\n",
      "   train acc: 0.832179\n",
      "   val loss: 0.708426\n",
      "   val acc: 0.751860\n",
      "Epoch 118 of 200 took 14.162147s\n",
      "   train loss: 0.430284\n",
      "   train acc: 0.829746\n",
      "   val loss: 0.727586\n",
      "   val acc: 0.762649\n",
      "Epoch 119 of 200 took 14.239391s\n",
      "   train loss: 0.426028\n",
      "   train acc: 0.833579\n",
      "   val loss: 0.668271\n",
      "   val acc: 0.752604\n",
      "Epoch 120 of 200 took 14.256927s\n",
      "   train loss: 0.421695\n",
      "   train acc: 0.837264\n",
      "   val loss: 0.673735\n",
      "   val acc: 0.755580\n",
      "Epoch 121 of 200 took 14.371404s\n",
      "   train loss: 0.425648\n",
      "   train acc: 0.832695\n",
      "   val loss: 0.697394\n",
      "   val acc: 0.761161\n",
      "Epoch 122 of 200 took 14.460159s\n",
      "   train loss: 0.436798\n",
      "   train acc: 0.832621\n",
      "   val loss: 0.648519\n",
      "   val acc: 0.754464\n",
      "Epoch 123 of 200 took 14.588047s\n",
      "   train loss: 0.511654\n",
      "   train acc: 0.810805\n",
      "   val loss: 0.846215\n",
      "   val acc: 0.738095\n",
      "Epoch 124 of 200 took 14.555093s\n",
      "   train loss: 0.459535\n",
      "   train acc: 0.828715\n",
      "   val loss: 0.632685\n",
      "   val acc: 0.759673\n",
      "Epoch 125 of 200 took 14.114482s\n",
      "   train loss: 0.425351\n",
      "   train acc: 0.832179\n",
      "   val loss: 0.670619\n",
      "   val acc: 0.756696\n",
      "Epoch 126 of 200 took 14.137783s\n",
      "   train loss: 0.443868\n",
      "   train acc: 0.829894\n",
      "   val loss: 0.653332\n",
      "   val acc: 0.750372\n",
      "Epoch 127 of 200 took 14.103360s\n",
      "   train loss: 0.442558\n",
      "   train acc: 0.829304\n",
      "   val loss: 0.703590\n",
      "   val acc: 0.735863\n",
      "Epoch 128 of 200 took 14.079539s\n",
      "   train loss: 0.416702\n",
      "   train acc: 0.835864\n",
      "   val loss: 0.714007\n",
      "   val acc: 0.748140\n",
      "Epoch 129 of 200 took 14.052554s\n",
      "   train loss: 0.422835\n",
      "   train acc: 0.834242\n",
      "   val loss: 0.731887\n",
      "   val acc: 0.735119\n",
      "Epoch 130 of 200 took 14.118838s\n",
      "   train loss: 0.419321\n",
      "   train acc: 0.834390\n",
      "   val loss: 0.692615\n",
      "   val acc: 0.764881\n",
      "Epoch 131 of 200 took 14.308090s\n",
      "   train loss: 0.434851\n",
      "   train acc: 0.834611\n",
      "   val loss: 0.635789\n",
      "   val acc: 0.766369\n",
      "Epoch 132 of 200 took 14.190911s\n",
      "   train loss: 0.426295\n",
      "   train acc: 0.833874\n",
      "   val loss: 0.686244\n",
      "   val acc: 0.745908\n",
      "Epoch 133 of 200 took 14.410691s\n",
      "   train loss: 0.426339\n",
      "   train acc: 0.833432\n",
      "   val loss: 0.683152\n",
      "   val acc: 0.747024\n",
      "Epoch 134 of 200 took 14.462852s\n",
      "   train loss: 0.430443\n",
      "   train acc: 0.832400\n",
      "   val loss: 0.686268\n",
      "   val acc: 0.761533\n",
      "Epoch 135 of 200 took 14.438071s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train loss: 0.421619\n",
      "   train acc: 0.834316\n",
      "   val loss: 0.679571\n",
      "   val acc: 0.765253\n",
      "Epoch 136 of 200 took 14.536453s\n",
      "   train loss: 0.416310\n",
      "   train acc: 0.835569\n",
      "   val loss: 0.676793\n",
      "   val acc: 0.752604\n",
      "Epoch 137 of 200 took 14.594765s\n",
      "   train loss: 0.415677\n",
      "   train acc: 0.833800\n",
      "   val loss: 0.747489\n",
      "   val acc: 0.754464\n",
      "Epoch 138 of 200 took 14.603447s\n",
      "   train loss: 0.416753\n",
      "   train acc: 0.836896\n",
      "   val loss: 0.683408\n",
      "   val acc: 0.748140\n",
      "Epoch 139 of 200 took 14.266177s\n",
      "   train loss: 0.426101\n",
      "   train acc: 0.830041\n",
      "   val loss: 0.737619\n",
      "   val acc: 0.757068\n",
      "Epoch 140 of 200 took 14.344370s\n",
      "   train loss: 0.438815\n",
      "   train acc: 0.829820\n",
      "   val loss: 0.670061\n",
      "   val acc: 0.729167\n",
      "Epoch 141 of 200 took 14.412518s\n",
      "   train loss: 0.415182\n",
      "   train acc: 0.835790\n",
      "   val loss: 0.703404\n",
      "   val acc: 0.741071\n",
      "Epoch 142 of 200 took 14.369385s\n",
      "   train loss: 0.410654\n",
      "   train acc: 0.837117\n",
      "   val loss: 0.671236\n",
      "   val acc: 0.760045\n",
      "Epoch 143 of 200 took 14.382010s\n",
      "   train loss: 0.434255\n",
      "   train acc: 0.831442\n",
      "   val loss: 0.707310\n",
      "   val acc: 0.725074\n",
      "Epoch 144 of 200 took 14.370769s\n",
      "   train loss: 0.434621\n",
      "   train acc: 0.832989\n",
      "   val loss: 0.672142\n",
      "   val acc: 0.730283\n",
      "Epoch 145 of 200 took 14.289525s\n",
      "   train loss: 0.420657\n",
      "   train acc: 0.836969\n",
      "   val loss: 0.683477\n",
      "   val acc: 0.736979\n",
      "Epoch 146 of 200 took 14.366769s\n",
      "   train loss: 0.451662\n",
      "   train acc: 0.827241\n",
      "   val loss: 0.798712\n",
      "   val acc: 0.765253\n",
      "Epoch 147 of 200 took 14.240262s\n",
      "   train loss: 0.415389\n",
      "   train acc: 0.831589\n",
      "   val loss: 0.734561\n",
      "   val acc: 0.756696\n",
      "Epoch 148 of 200 took 14.365883s\n",
      "   train loss: 0.422561\n",
      "   train acc: 0.833432\n",
      "   val loss: 0.705125\n",
      "   val acc: 0.742560\n",
      "Epoch 149 of 200 took 14.354130s\n",
      "   train loss: 0.420514\n",
      "   train acc: 0.838149\n",
      "   val loss: 0.669294\n",
      "   val acc: 0.738095\n",
      "Epoch 150 of 200 took 14.501062s\n",
      "   train loss: 0.446890\n",
      "   train acc: 0.826651\n",
      "   val loss: 0.671131\n",
      "   val acc: 0.734375\n",
      "Epoch 151 of 200 took 14.500785s\n",
      "   train loss: 0.412909\n",
      "   train acc: 0.835348\n",
      "   val loss: 0.730725\n",
      "   val acc: 0.751116\n",
      "Epoch 152 of 200 took 14.193224s\n",
      "   train loss: 0.408978\n",
      "   train acc: 0.839033\n",
      "   val loss: 0.707427\n",
      "   val acc: 0.758557\n",
      "Epoch 153 of 200 took 14.343227s\n",
      "   train loss: 0.414258\n",
      "   train acc: 0.836675\n",
      "   val loss: 0.667027\n",
      "   val acc: 0.756324\n",
      "Epoch 154 of 200 took 14.332881s\n",
      "   train loss: 0.420449\n",
      "   train acc: 0.835864\n",
      "   val loss: 0.682187\n",
      "   val acc: 0.744048\n",
      "Epoch 155 of 200 took 14.378778s\n",
      "   train loss: 0.474803\n",
      "   train acc: 0.820607\n",
      "   val loss: 0.655564\n",
      "   val acc: 0.758185\n",
      "Epoch 156 of 200 took 14.475660s\n",
      "   train loss: 0.418102\n",
      "   train acc: 0.836896\n",
      "   val loss: 0.692546\n",
      "   val acc: 0.744048\n",
      "Epoch 157 of 200 took 14.495839s\n",
      "   train loss: 0.426749\n",
      "   train acc: 0.830926\n",
      "   val loss: 0.759394\n",
      "   val acc: 0.754092\n",
      "Epoch 158 of 200 took 14.348374s\n",
      "   train loss: 0.415566\n",
      "   train acc: 0.840065\n",
      "   val loss: 0.664086\n",
      "   val acc: 0.756324\n",
      "Epoch 159 of 200 took 14.389020s\n",
      "   train loss: 0.427712\n",
      "   train acc: 0.832252\n",
      "   val loss: 0.742490\n",
      "   val acc: 0.751860\n",
      "Epoch 160 of 200 took 14.266268s\n",
      "   train loss: 0.427626\n",
      "   train acc: 0.836306\n",
      "   val loss: 0.669736\n",
      "   val acc: 0.763021\n",
      "Epoch 161 of 200 took 14.347690s\n",
      "   train loss: 0.412049\n",
      "   train acc: 0.838075\n",
      "   val loss: 0.691767\n",
      "   val acc: 0.751116\n",
      "Epoch 162 of 200 took 14.476193s\n",
      "   train loss: 0.412925\n",
      "   train acc: 0.833874\n",
      "   val loss: 0.726249\n",
      "   val acc: 0.746652\n",
      "Epoch 163 of 200 took 14.338716s\n",
      "   train loss: 0.431728\n",
      "   train acc: 0.834537\n",
      "   val loss: 0.627116\n",
      "   val acc: 0.765997\n",
      "Epoch 164 of 200 took 14.319109s\n",
      "   train loss: 0.418549\n",
      "   train acc: 0.835274\n",
      "   val loss: 0.675793\n",
      "   val acc: 0.751860\n",
      "Epoch 165 of 200 took 14.227242s\n",
      "   train loss: 0.420756\n",
      "   train acc: 0.840654\n",
      "   val loss: 0.679095\n",
      "   val acc: 0.759301\n",
      "Epoch 166 of 200 took 14.395068s\n",
      "   train loss: 0.417075\n",
      "   train acc: 0.837043\n",
      "   val loss: 0.671828\n",
      "   val acc: 0.746652\n",
      "Epoch 167 of 200 took 14.388772s\n",
      "   train loss: 0.437928\n",
      "   train acc: 0.829009\n",
      "   val loss: 0.759812\n",
      "   val acc: 0.752604\n",
      "Epoch 168 of 200 took 14.389179s\n",
      "   train loss: 0.424411\n",
      "   train acc: 0.834021\n",
      "   val loss: 0.669457\n",
      "   val acc: 0.749256\n",
      "Epoch 169 of 200 took 14.316262s\n",
      "   train loss: 0.413070\n",
      "   train acc: 0.838812\n",
      "   val loss: 0.657494\n",
      "   val acc: 0.767113\n",
      "Epoch 170 of 200 took 14.436116s\n",
      "   train loss: 0.405253\n",
      "   train acc: 0.837854\n",
      "   val loss: 0.778809\n",
      "   val acc: 0.741443\n",
      "Epoch 171 of 200 took 14.251693s\n",
      "   train loss: 0.407266\n",
      "   train acc: 0.842202\n",
      "   val loss: 0.644531\n",
      "   val acc: 0.764881\n",
      "Epoch 172 of 200 took 14.341426s\n",
      "   train loss: 0.426746\n",
      "   train acc: 0.836232\n",
      "   val loss: 0.745187\n",
      "   val acc: 0.760417\n",
      "Epoch 173 of 200 took 14.357100s\n",
      "   train loss: 0.426562\n",
      "   train acc: 0.832547\n",
      "   val loss: 0.674967\n",
      "   val acc: 0.734747\n",
      "Epoch 174 of 200 took 14.448441s\n",
      "   train loss: 0.463817\n",
      "   train acc: 0.824219\n",
      "   val loss: 0.857910\n",
      "   val acc: 0.745164\n",
      "Epoch 175 of 200 took 14.435995s\n",
      "   train loss: 0.425431\n",
      "   train acc: 0.839180\n",
      "   val loss: 0.656564\n",
      "   val acc: 0.753348\n",
      "Epoch 176 of 200 took 14.263681s\n",
      "   train loss: 0.406586\n",
      "   train acc: 0.837338\n",
      "   val loss: 0.712286\n",
      "   val acc: 0.746280\n",
      "Epoch 177 of 200 took 14.391582s\n",
      "   train loss: 0.420691\n",
      "   train acc: 0.835643\n",
      "   val loss: 0.699416\n",
      "   val acc: 0.765625\n",
      "Epoch 178 of 200 took 14.391030s\n",
      "   train loss: 0.419846\n",
      "   train acc: 0.834906\n",
      "   val loss: 0.766532\n",
      "   val acc: 0.749628\n",
      "Epoch 179 of 200 took 14.405566s\n",
      "   train loss: 0.504328\n",
      "   train acc: 0.821418\n",
      "   val loss: 0.936547\n",
      "   val acc: 0.730655\n",
      "Epoch 180 of 200 took 14.425617s\n",
      "   train loss: 0.402920\n",
      "   train acc: 0.841318\n",
      "   val loss: 0.670968\n",
      "   val acc: 0.755580\n",
      "Epoch 181 of 200 took 14.425373s\n",
      "   train loss: 0.407805\n",
      "   train acc: 0.848835\n",
      "   val loss: 0.651693\n",
      "   val acc: 0.758185\n",
      "Epoch 182 of 200 took 14.447854s\n",
      "   train loss: 0.400856\n",
      "   train acc: 0.841539\n",
      "   val loss: 0.674072\n",
      "   val acc: 0.762649\n",
      "Epoch 183 of 200 took 14.435066s\n",
      "   train loss: 0.409221\n",
      "   train acc: 0.838886\n",
      "   val loss: 0.735421\n",
      "   val acc: 0.757812\n",
      "Epoch 184 of 200 took 14.489507s\n",
      "   train loss: 0.483514\n",
      "   train acc: 0.820534\n",
      "   val loss: 0.680594\n",
      "   val acc: 0.720238\n",
      "Epoch 185 of 200 took 14.427919s\n",
      "   train loss: 0.421928\n",
      "   train acc: 0.830557\n",
      "   val loss: 0.734770\n",
      "   val acc: 0.761533\n",
      "Epoch 186 of 200 took 14.529815s\n",
      "   train loss: 0.401199\n",
      "   train acc: 0.843603\n",
      "   val loss: 0.696475\n",
      "   val acc: 0.762277\n",
      "Epoch 187 of 200 took 14.543039s\n",
      "   train loss: 0.407722\n",
      "   train acc: 0.841760\n",
      "   val loss: 0.653169\n",
      "   val acc: 0.752232\n",
      "Epoch 188 of 200 took 14.557847s\n",
      "   train loss: 0.411999\n",
      "   train acc: 0.838591\n",
      "   val loss: 0.690569\n",
      "   val acc: 0.732143\n",
      "Epoch 189 of 200 took 14.464695s\n",
      "   train loss: 0.424111\n",
      "   train acc: 0.834316\n",
      "   val loss: 0.767206\n",
      "   val acc: 0.764509\n",
      "Epoch 190 of 200 took 14.470448s\n",
      "   train loss: 0.396800\n",
      "   train acc: 0.843824\n",
      "   val loss: 0.740467\n",
      "   val acc: 0.745908\n",
      "Epoch 191 of 200 took 14.352172s\n",
      "   train loss: 0.428944\n",
      "   train acc: 0.836896\n",
      "   val loss: 0.773112\n",
      "   val acc: 0.753348\n",
      "Epoch 192 of 200 took 14.289042s\n",
      "   train loss: 0.414917\n",
      "   train acc: 0.842939\n",
      "   val loss: 0.636870\n",
      "   val acc: 0.763021\n",
      "Epoch 193 of 200 took 14.331516s\n",
      "   train loss: 0.409663\n",
      "   train acc: 0.842644\n",
      "   val loss: 0.684001\n",
      "   val acc: 0.748140\n",
      "Epoch 194 of 200 took 14.329207s\n",
      "   train loss: 0.399046\n",
      "   train acc: 0.840728\n",
      "   val loss: 0.712972\n",
      "   val acc: 0.753720\n",
      "Epoch 195 of 200 took 14.353199s\n",
      "   train loss: 0.415039\n",
      "   train acc: 0.839696\n",
      "   val loss: 0.633603\n",
      "   val acc: 0.765253\n",
      "Epoch 196 of 200 took 14.403080s\n",
      "   train loss: 0.400533\n",
      "   train acc: 0.839549\n",
      "   val loss: 0.694173\n",
      "   val acc: 0.743676\n",
      "Epoch 197 of 200 took 14.385883s\n",
      "   train loss: 0.423616\n",
      "   train acc: 0.834832\n",
      "   val loss: 0.651542\n",
      "   val acc: 0.755208\n",
      "Epoch 198 of 200 took 14.314452s\n",
      "   train loss: 0.401773\n",
      "   train acc: 0.836748\n",
      "   val loss: 0.744048\n",
      "   val acc: 0.746652\n",
      "Epoch 199 of 200 took 14.282953s\n",
      "   train loss: 0.438014\n",
      "   train acc: 0.832400\n",
      "   val loss: 0.740211\n",
      "   val acc: 0.769345\n",
      "Epoch 200 of 200 took 14.373192s\n",
      "   train loss: 0.401001\n",
      "   train acc: 0.839991\n",
      "   val loss: 0.702079\n",
      "   val acc: 0.739211\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "n_epoch = 200\n",
    "print_freq = 1\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "        sess.run(train_op, feed_dict={x: X_train_a, y_: y_train_a})\n",
    "\n",
    "    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
    "        print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
    "        train_loss, train_acc, n_batch = 0, 0, 0\n",
    "        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n",
    "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_train_a, y_: y_train_a})\n",
    "            train_loss += err\n",
    "            train_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "        print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    "        val_loss, val_acc, n_batch = 0, 0, 0\n",
    "        for X_val_a, y_val_a in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=True):\n",
    "            err, ac = sess.run([cost_test, acc], feed_dict={x: X_val_a, y_: y_val_a})\n",
    "            val_loss += err\n",
    "            val_acc += ac\n",
    "            n_batch += 1\n",
    "        print(\"   val loss: %f\" % (val_loss / n_batch))\n",
    "        print(\"   val acc: %f\" % (val_acc / n_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test loss: 0.711607\n",
      "   test acc: 0.692419\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, n_batch = 0, 0, 0\n",
    "for X_test_a, y_test_a in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=True):\n",
    "    err, ac = sess.run([cost_test, acc], feed_dict={x: X_test_a, y_: y_test_a})\n",
    "    test_loss += err\n",
    "    test_acc += ac\n",
    "    n_batch += 1\n",
    "print(\"   test loss: %f\" % (test_loss / n_batch))\n",
    "print(\"   test acc: %f\" % (test_acc / n_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] [*] model2d.npz saved\n"
     ]
    }
   ],
   "source": [
    "tl.files.save_npz(net_train.all_params, name='model2d.npz')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] [*] Load model2d.npz SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorlayer.layers.core.DenseLayer at 0x265437a4828>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.files.load_and_assign_npz(sess=sess, name='model2d.npz', network=net_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling TensorRider Using the Generated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython import display\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import socket\n",
    "import threading\n",
    "from time import ctime,sleep\n",
    "import string\n",
    "\n",
    "remoteImage = np.array([])\n",
    "stream = urllib.request.urlopen('http://192.168.73.73:8080/?action=stream&ignored.mjpg')\n",
    "bytes = bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_Refreshing_Thread():\n",
    "    global remoteImage\n",
    "    global stream\n",
    "    global bytes\n",
    "    while True:\n",
    "        bytes += stream.read(1024)\n",
    "        a = bytes.find(b'\\xff\\xd8')\n",
    "        b = bytes.find(b'\\xff\\xd9')\n",
    "        if a != -1 and b != -1:\n",
    "            jpg = bytes[a:b+2]\n",
    "            bytes = bytes[b+2:]\n",
    "            i = cv2.imdecode(np.fromstring(jpg, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "            i = rgb2mono(i)\n",
    "            remoteImage = i.reshape((1,4800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Controlling_Thread():\n",
    "    global remoteImage\n",
    "    addr=('192.168.73.73',51423)\n",
    "    s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n",
    "    while True:\n",
    "        direction = tl.utils.predict(sess, net_train, remoteImage, x, y_op, batch_size=None)\n",
    "        msgCtrl_Udp = str(25) + \",\" + str((direction[0] - 3) * 15)\n",
    "#         print(msgCtrl_Udp)\n",
    "        s.sendto(msgCtrl_Udp.encode('utf-8'), addr)\n",
    "        sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_op = tf.argmax(tf.nn.softmax(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "RefreshImageThread = threading.Thread(target = Image_Refreshing_Thread)\n",
    "RefreshImageThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-17-541ca816504e>\", line 6, in Controlling_Thread\n",
      "    direction = tl.utils.predict(sess, net_train, remoteImage, x, y_op, batch_size=None)\n",
      "  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorlayer\\utils.py\", line 315, in predict\n",
      "    return sess.run(y_op, feed_dict=feed_dict)\n",
      "  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1113, in _run\n",
      "    str(subfeed_t.get_shape())))\n",
      "ValueError: Cannot feed value of shape (1, 4800) for Tensor 'Placeholder:0', which has shape '(128, 60, 80, 1)'\n",
      "\n",
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "ControllingThread = threading.Thread(target = Controlling_Thread)\n",
    "ControllingThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
