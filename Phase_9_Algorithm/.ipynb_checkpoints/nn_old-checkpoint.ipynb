{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRider Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rgb2mono(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    # mono = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    mono = 0.5 * r + 0.25 * g + 0.25 * b\n",
    "    return mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareDataArrays(iterator):\n",
    "    X = np.zeros(4800)\n",
    "    y = np.zeros(1)\n",
    "    recordCounter = 0;\n",
    "\n",
    "    for string_record in iterator:\n",
    "        recordCounter += 1\n",
    "\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(string_record)\n",
    "        imageString = (example.features.feature['image'].bytes_list.value[0])\n",
    "        label = (example.features.feature['label'].int64_list.value[0])\n",
    "\n",
    "        image = np.fromstring(imageString, dtype=np.uint8)\n",
    "        image = image.reshape((60, 80, 3))\n",
    "        image = rgb2mono(image)\n",
    "        image = image.reshape((4800))\n",
    "\n",
    "        X = np.vstack((X,image))\n",
    "        y = np.append(y,label)\n",
    "        if recordCounter % 100 == 0:\n",
    "            print(recordCounter,end = '->')\n",
    "\n",
    "    y = y.reshape((recordCounter + 1,))\n",
    "#     y = np.round(y / 7) #Downsampling\n",
    "#     y = y + 6\n",
    "    y = np.round(y / 12)\n",
    "    y = y + 3\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainIterator = tf.python_io.tf_record_iterator(path=\"train.tfrecords\")\n",
    "valIterator = tf.python_io.tf_record_iterator(path=\"val.tfrecords\")\n",
    "testIterator = tf.python_io.tf_record_iterator(path=\"test.tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecords -> Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train...\n",
      "100->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200->300->400->500->600->"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain...\")\n",
    "X_train, y_train = prepareDataArrays(trainIterator)\n",
    "print(\"\\nVal...\")\n",
    "X_val, y_val = prepareDataArrays(valIterator)\n",
    "print(\"\\nTest...\")\n",
    "X_test, y_test = prepareDataArrays(testIterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays -> NPZ File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez(\"arrays.npz\", xtr = X_train, ytr = y_train, xval = X_val, yval = y_val, xt = X_test, yt = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPZ File -> Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npRecall = np.load(\"arrays.npz\")\n",
    "X_train = npRecall[\"xtr\"]\n",
    "y_train = npRecall[\"ytr\"]\n",
    "X_val = npRecall[\"xval\"]\n",
    "y_val = npRecall[\"yval\"]\n",
    "X_test = npRecall[\"xt\"]\n",
    "y_test = npRecall[\"yt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 4800], name='x')\n",
    "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  input: (?, 4800)\n",
      "[TL] DropoutLayer drop1: keep:0.800000 is_fix:False\n",
      "[TL] DenseLayer  relu1: 2048 relu\n",
      "[TL] DenseLayer  relu2: 1024 relu\n",
      "[TL] DenseLayer  relu3: 1024 relu\n",
      "[TL] DenseLayer  relu4: 512 relu\n",
      "[TL] DenseLayer  output: 7 identity\n"
     ]
    }
   ],
   "source": [
    "network = tl.layers.InputLayer(x, name='input')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
    "network = tl.layers.DenseLayer(network, 2048, tf.nn.relu, name='relu1')\n",
    "# network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
    "network = tl.layers.DenseLayer(network, 1024, tf.nn.relu, name='relu2')\n",
    "# network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n",
    "network = tl.layers.DenseLayer(network, 1024, tf.nn.relu, name='relu3')\n",
    "network = tl.layers.DenseLayer(network, 512, tf.nn.relu, name='relu4')\n",
    "# network = tl.layers.DenseLayer(network, n_units=13, act=tf.identity, name='output')\n",
    "network = tl.layers.DenseLayer(network, n_units=7, act=tf.identity, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = network.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='cost')\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "y_op = tf.argmax(tf.nn.softmax(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_params = network.all_params\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   param   0: relu1/W:0            (4800, 2048)       float32_ref (mean: -4.6321041736518964e-05, median: -5.021012111683376e-05, std: 0.08795594424009323)   \n",
      "[TL]   param   1: relu1/b:0            (2048,)            float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   2: relu2/W:0            (2048, 1024)       float32_ref (mean: 2.2710790290147997e-05, median: 0.00011796068429248407, std: 0.08794547617435455)   \n",
      "[TL]   param   3: relu2/b:0            (1024,)            float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   4: relu3/W:0            (1024, 1024)       float32_ref (mean: -6.286404641286936e-06, median: 6.734003545716405e-05, std: 0.08792474120855331)   \n",
      "[TL]   param   5: relu3/b:0            (1024,)            float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   6: relu4/W:0            (1024, 512)        float32_ref (mean: -1.4396109691006131e-06, median: 7.763020403217524e-05, std: 0.08803294599056244)   \n",
      "[TL]   param   7: relu4/b:0            (512,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   8: output/W:0           (512, 7)           float32_ref (mean: 0.0014812672743573785, median: 0.0006800247356295586, std: 0.08885616809129715)   \n",
      "[TL]   param   9: output/b:0           (7,)               float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   num of params: 13508615\n"
     ]
    }
   ],
   "source": [
    "network.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   layer   0: drop1/mul:0          (?, 4800)          float32\n",
      "[TL]   layer   1: relu1/Relu:0         (?, 2048)          float32\n",
      "[TL]   layer   2: relu2/Relu:0         (?, 1024)          float32\n",
      "[TL]   layer   3: relu3/Relu:0         (?, 1024)          float32\n",
      "[TL]   layer   4: relu4/Relu:0         (?, 512)           float32\n",
      "[TL]   layer   5: output/Identity:0    (?, 7)             float32\n"
     ]
    }
   ],
   "source": [
    "network.print_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Setting up TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_summ = tf.summary.scalar('acc', acc)  \n",
    "cost_summ = tf.summary.scalar('cost', cost)  \n",
    "summary = tf.summary.merge_all()  \n",
    "writer = tf.summary.FileWriter('./logs')  \n",
    "writer.add_graph(sess.graph) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] Start training the network ...\n",
      "[TL] Epoch 1 of 500 took 2.645558s\n",
      "[TL]    val loss: 510.058314\n",
      "[TL]    val acc: 0.731176\n",
      "[TL] Epoch 5 of 500 took 2.417820s\n",
      "[TL]    val loss: 229.320870\n",
      "[TL]    val acc: 0.746471\n",
      "[TL] Epoch 10 of 500 took 2.426727s\n",
      "[TL]    val loss: 103.221599\n",
      "[TL]    val acc: 0.660000\n",
      "[TL] Epoch 15 of 500 took 2.437702s\n",
      "[TL]    val loss: 66.788516\n",
      "[TL]    val acc: 0.740000\n",
      "[TL] Epoch 20 of 500 took 2.364301s\n",
      "[TL]    val loss: 25.613014\n",
      "[TL]    val acc: 0.770000\n",
      "[TL] Epoch 25 of 500 took 2.445139s\n",
      "[TL]    val loss: 67.697136\n",
      "[TL]    val acc: 0.562941\n",
      "[TL] Epoch 30 of 500 took 2.393829s\n",
      "[TL]    val loss: 18.173607\n",
      "[TL]    val acc: 0.731765\n",
      "[TL] Epoch 35 of 500 took 2.423412s\n",
      "[TL]    val loss: 29.339399\n",
      "[TL]    val acc: 0.600000\n",
      "[TL] Epoch 40 of 500 took 2.364823s\n",
      "[TL]    val loss: 28.156965\n",
      "[TL]    val acc: 0.534706\n",
      "[TL] Epoch 45 of 500 took 2.417533s\n",
      "[TL]    val loss: 15.318660\n",
      "[TL]    val acc: 0.671765\n",
      "[TL] Epoch 50 of 500 took 2.399176s\n",
      "[TL]    val loss: 20.664268\n",
      "[TL]    val acc: 0.567647\n",
      "[TL] Epoch 55 of 500 took 2.465489s\n",
      "[TL]    val loss: 7.601790\n",
      "[TL]    val acc: 0.757647\n",
      "[TL] Epoch 60 of 500 took 2.410475s\n",
      "[TL]    val loss: 7.850280\n",
      "[TL]    val acc: 0.705294\n",
      "[TL] Epoch 65 of 500 took 2.495087s\n",
      "[TL]    val loss: 7.332313\n",
      "[TL]    val acc: 0.621176\n",
      "[TL] Epoch 70 of 500 took 2.500329s\n",
      "[TL]    val loss: 3.940217\n",
      "[TL]    val acc: 0.757647\n",
      "[TL] Epoch 75 of 500 took 2.523109s\n",
      "[TL]    val loss: 10.912930\n",
      "[TL]    val acc: 0.667059\n",
      "[TL] Epoch 80 of 500 took 2.498435s\n",
      "[TL]    val loss: 9.877615\n",
      "[TL]    val acc: 0.764706\n",
      "[TL] Epoch 85 of 500 took 2.392646s\n",
      "[TL]    val loss: 5.630651\n",
      "[TL]    val acc: 0.587647\n",
      "[TL] Epoch 90 of 500 took 2.456682s\n",
      "[TL]    val loss: 6.627220\n",
      "[TL]    val acc: 0.700588\n",
      "[TL] Epoch 95 of 500 took 2.573521s\n",
      "[TL]    val loss: 8.047617\n",
      "[TL]    val acc: 0.622353\n",
      "[TL] Epoch 100 of 500 took 2.488638s\n",
      "[TL]    val loss: 9.058798\n",
      "[TL]    val acc: 0.622353\n",
      "[TL] Epoch 105 of 500 took 2.520179s\n",
      "[TL]    val loss: 9.173403\n",
      "[TL]    val acc: 0.512941\n",
      "[TL] Epoch 110 of 500 took 2.562072s\n",
      "[TL]    val loss: 3.961896\n",
      "[TL]    val acc: 0.584118\n",
      "[TL] Epoch 115 of 500 took 2.604767s\n",
      "[TL]    val loss: 5.726129\n",
      "[TL]    val acc: 0.664706\n",
      "[TL] Epoch 120 of 500 took 2.542851s\n",
      "[TL]    val loss: 4.379606\n",
      "[TL]    val acc: 0.711765\n",
      "[TL] Epoch 125 of 500 took 2.536805s\n",
      "[TL]    val loss: 4.820790\n",
      "[TL]    val acc: 0.652353\n",
      "[TL] Epoch 130 of 500 took 2.536750s\n",
      "[TL]    val loss: 3.933930\n",
      "[TL]    val acc: 0.572353\n",
      "[TL] Epoch 135 of 500 took 2.533545s\n",
      "[TL]    val loss: 2.972678\n",
      "[TL]    val acc: 0.737059\n",
      "[TL] Epoch 140 of 500 took 2.433711s\n",
      "[TL]    val loss: 3.380756\n",
      "[TL]    val acc: 0.542941\n",
      "[TL] Epoch 145 of 500 took 2.378460s\n",
      "[TL]    val loss: 5.175468\n",
      "[TL]    val acc: 0.581176\n",
      "[TL] Epoch 150 of 500 took 2.438049s\n",
      "[TL]    val loss: 3.903676\n",
      "[TL]    val acc: 0.576471\n",
      "[TL] Epoch 155 of 500 took 2.416857s\n",
      "[TL]    val loss: 2.026564\n",
      "[TL]    val acc: 0.750000\n",
      "[TL] Epoch 160 of 500 took 2.524734s\n",
      "[TL]    val loss: 3.674225\n",
      "[TL]    val acc: 0.577647\n",
      "[TL] Epoch 165 of 500 took 2.467569s\n",
      "[TL]    val loss: 2.838374\n",
      "[TL]    val acc: 0.788824\n",
      "[TL] Epoch 170 of 500 took 2.438457s\n",
      "[TL]    val loss: 1.367546\n",
      "[TL]    val acc: 0.748235\n",
      "[TL] Epoch 175 of 500 took 2.441527s\n",
      "[TL]    val loss: 4.457979\n",
      "[TL]    val acc: 0.528824\n",
      "[TL] Epoch 180 of 500 took 2.413508s\n",
      "[TL]    val loss: 1.977998\n",
      "[TL]    val acc: 0.727647\n",
      "[TL] Epoch 185 of 500 took 2.465558s\n",
      "[TL]    val loss: 1.835176\n",
      "[TL]    val acc: 0.733529\n",
      "[TL] Epoch 190 of 500 took 2.447513s\n",
      "[TL]    val loss: 2.759325\n",
      "[TL]    val acc: 0.627647\n",
      "[TL] Epoch 195 of 500 took 2.556407s\n",
      "[TL]    val loss: 3.613993\n",
      "[TL]    val acc: 0.592353\n",
      "[TL] Epoch 200 of 500 took 2.523987s\n",
      "[TL]    val loss: 2.768765\n",
      "[TL]    val acc: 0.678824\n",
      "[TL] Epoch 205 of 500 took 2.544489s\n",
      "[TL]    val loss: 2.266978\n",
      "[TL]    val acc: 0.575882\n",
      "[TL] Epoch 210 of 500 took 2.648592s\n",
      "[TL]    val loss: 1.313913\n",
      "[TL]    val acc: 0.697059\n",
      "[TL] Epoch 215 of 500 took 2.454865s\n",
      "[TL]    val loss: 2.618457\n",
      "[TL]    val acc: 0.574118\n",
      "[TL] Epoch 220 of 500 took 2.589902s\n",
      "[TL]    val loss: 2.302521\n",
      "[TL]    val acc: 0.556471\n",
      "[TL] Epoch 225 of 500 took 2.545105s\n",
      "[TL]    val loss: 0.871256\n",
      "[TL]    val acc: 0.788235\n",
      "[TL] Epoch 230 of 500 took 2.555132s\n",
      "[TL]    val loss: 1.419230\n",
      "[TL]    val acc: 0.661176\n",
      "[TL] Epoch 235 of 500 took 2.482080s\n",
      "[TL]    val loss: 1.142475\n",
      "[TL]    val acc: 0.707647\n",
      "[TL] Epoch 240 of 500 took 2.424297s\n",
      "[TL]    val loss: 1.074586\n",
      "[TL]    val acc: 0.708235\n",
      "[TL] Epoch 245 of 500 took 2.354233s\n",
      "[TL]    val loss: 0.867304\n",
      "[TL]    val acc: 0.757059\n",
      "[TL] Epoch 250 of 500 took 2.464063s\n",
      "[TL]    val loss: 0.904281\n",
      "[TL]    val acc: 0.771176\n",
      "[TL] Epoch 255 of 500 took 2.399155s\n",
      "[TL]    val loss: 1.044727\n",
      "[TL]    val acc: 0.711176\n",
      "[TL] Epoch 260 of 500 took 2.413961s\n",
      "[TL]    val loss: 0.790704\n",
      "[TL]    val acc: 0.746471\n",
      "[TL] Epoch 265 of 500 took 2.468755s\n",
      "[TL]    val loss: 0.880232\n",
      "[TL]    val acc: 0.690588\n",
      "[TL] Epoch 270 of 500 took 2.493568s\n",
      "[TL]    val loss: 1.001373\n",
      "[TL]    val acc: 0.792353\n",
      "[TL] Epoch 275 of 500 took 2.408880s\n",
      "[TL]    val loss: 1.107257\n",
      "[TL]    val acc: 0.641176\n",
      "[TL] Epoch 280 of 500 took 2.411558s\n",
      "[TL]    val loss: 0.727580\n",
      "[TL]    val acc: 0.751176\n",
      "[TL] Epoch 285 of 500 took 2.474585s\n",
      "[TL]    val loss: 1.044812\n",
      "[TL]    val acc: 0.731765\n",
      "[TL] Epoch 290 of 500 took 2.434474s\n",
      "[TL]    val loss: 0.746411\n",
      "[TL]    val acc: 0.790000\n",
      "[TL] Epoch 295 of 500 took 2.386343s\n",
      "[TL]    val loss: 2.005534\n",
      "[TL]    val acc: 0.541176\n",
      "[TL] Epoch 300 of 500 took 2.417400s\n",
      "[TL]    val loss: 0.709857\n",
      "[TL]    val acc: 0.769412\n",
      "[TL] Epoch 305 of 500 took 2.408409s\n",
      "[TL]    val loss: 0.697079\n",
      "[TL]    val acc: 0.791765\n",
      "[TL] Epoch 310 of 500 took 2.481570s\n",
      "[TL]    val loss: 0.843704\n",
      "[TL]    val acc: 0.737647\n",
      "[TL] Epoch 315 of 500 took 2.454531s\n",
      "[TL]    val loss: 1.007371\n",
      "[TL]    val acc: 0.612941\n",
      "[TL] Epoch 320 of 500 took 2.429466s\n",
      "[TL]    val loss: 0.880323\n",
      "[TL]    val acc: 0.713529\n",
      "[TL] Epoch 325 of 500 took 2.420426s\n",
      "[TL]    val loss: 0.689269\n",
      "[TL]    val acc: 0.768235\n",
      "[TL] Epoch 330 of 500 took 2.440493s\n",
      "[TL]    val loss: 0.632880\n",
      "[TL]    val acc: 0.784118\n",
      "[TL] Epoch 335 of 500 took 2.433475s\n",
      "[TL]    val loss: 0.910599\n",
      "[TL]    val acc: 0.705294\n",
      "[TL] Epoch 340 of 500 took 2.418412s\n",
      "[TL]    val loss: 1.061244\n",
      "[TL]    val acc: 0.672353\n",
      "[TL] Epoch 345 of 500 took 2.507673s\n",
      "[TL]    val loss: 0.646841\n",
      "[TL]    val acc: 0.787059\n",
      "[TL] Epoch 350 of 500 took 2.499651s\n",
      "[TL]    val loss: 0.572215\n",
      "[TL]    val acc: 0.792353\n",
      "[TL] Epoch 355 of 500 took 2.440493s\n",
      "[TL]    val loss: 0.535227\n",
      "[TL]    val acc: 0.801176\n",
      "[TL] Epoch 360 of 500 took 2.442125s\n",
      "[TL]    val loss: 0.723889\n",
      "[TL]    val acc: 0.749412\n",
      "[TL] Epoch 365 of 500 took 2.423449s\n",
      "[TL]    val loss: 0.599778\n",
      "[TL]    val acc: 0.778824\n",
      "[TL] Epoch 370 of 500 took 2.446511s\n",
      "[TL]    val loss: 1.084305\n",
      "[TL]    val acc: 0.545882\n",
      "[TL] Epoch 375 of 500 took 2.445506s\n",
      "[TL]    val loss: 0.543629\n",
      "[TL]    val acc: 0.797647\n",
      "[TL] Epoch 380 of 500 took 2.447511s\n",
      "[TL]    val loss: 0.616200\n",
      "[TL]    val acc: 0.789412\n",
      "[TL] Epoch 385 of 500 took 2.436482s\n",
      "[TL]    val loss: 0.586315\n",
      "[TL]    val acc: 0.775882\n",
      "[TL] Epoch 390 of 500 took 2.483640s\n",
      "[TL]    val loss: 0.563087\n",
      "[TL]    val acc: 0.782941\n",
      "[TL] Epoch 395 of 500 took 2.468570s\n",
      "[TL]    val loss: 0.519197\n",
      "[TL]    val acc: 0.804118\n",
      "[TL] Epoch 400 of 500 took 2.594905s\n",
      "[TL]    val loss: 0.582787\n",
      "[TL]    val acc: 0.790588\n",
      "[TL] Epoch 405 of 500 took 2.513688s\n",
      "[TL]    val loss: 0.690370\n",
      "[TL]    val acc: 0.734118\n",
      "[TL] Epoch 410 of 500 took 2.479580s\n",
      "[TL]    val loss: 0.546214\n",
      "[TL]    val acc: 0.791765\n",
      "[TL] Epoch 415 of 500 took 2.437513s\n",
      "[TL]    val loss: 0.547798\n",
      "[TL]    val acc: 0.792353\n",
      "[TL] Epoch 420 of 500 took 2.465592s\n",
      "[TL]    val loss: 0.718991\n",
      "[TL]    val acc: 0.679412\n",
      "[TL] Epoch 425 of 500 took 2.585881s\n",
      "[TL]    val loss: 0.543274\n",
      "[TL]    val acc: 0.792353\n",
      "[TL] Epoch 430 of 500 took 2.447480s\n",
      "[TL]    val loss: 0.615803\n",
      "[TL]    val acc: 0.767647\n",
      "[TL] Epoch 435 of 500 took 2.436480s\n",
      "[TL]    val loss: 0.598474\n",
      "[TL]    val acc: 0.760000\n",
      "[TL] Epoch 440 of 500 took 2.420439s\n",
      "[TL]    val loss: 0.582354\n",
      "[TL]    val acc: 0.774706\n",
      "[TL] Epoch 445 of 500 took 2.441497s\n",
      "[TL]    val loss: 0.549869\n",
      "[TL]    val acc: 0.775294\n",
      "[TL] Epoch 450 of 500 took 2.414405s\n",
      "[TL]    val loss: 0.642084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]    val acc: 0.744706\n",
      "[TL] Epoch 455 of 500 took 2.449519s\n",
      "[TL]    val loss: 0.512948\n",
      "[TL]    val acc: 0.794706\n",
      "[TL] Epoch 460 of 500 took 2.414457s\n",
      "[TL]    val loss: 0.543436\n",
      "[TL]    val acc: 0.785882\n",
      "[TL] Epoch 465 of 500 took 2.462675s\n",
      "[TL]    val loss: 0.534590\n",
      "[TL]    val acc: 0.790588\n",
      "[TL] Epoch 470 of 500 took 2.460549s\n",
      "[TL]    val loss: 0.512057\n",
      "[TL]    val acc: 0.798235\n",
      "[TL] Epoch 475 of 500 took 2.532735s\n",
      "[TL]    val loss: 0.520691\n",
      "[TL]    val acc: 0.805882\n",
      "[TL] Epoch 480 of 500 took 2.482417s\n",
      "[TL]    val loss: 0.537217\n",
      "[TL]    val acc: 0.784706\n",
      "[TL] Epoch 485 of 500 took 2.455504s\n",
      "[TL]    val loss: 0.522622\n",
      "[TL]    val acc: 0.792941\n",
      "[TL] Epoch 490 of 500 took 2.422447s\n",
      "[TL]    val loss: 0.509262\n",
      "[TL]    val acc: 0.807647\n",
      "[TL] Epoch 495 of 500 took 2.451523s\n",
      "[TL]    val loss: 0.524422\n",
      "[TL]    val acc: 0.804706\n",
      "[TL] Epoch 500 of 500 took 2.415392s\n",
      "[TL]    val loss: 0.516925\n",
      "[TL]    val acc: 0.805294\n",
      "[TL] Total training time: 1247.000602s\n"
     ]
    }
   ],
   "source": [
    "tl.utils.fit(\n",
    "    sess, network, train_op, cost, X_train, y_train, x, y_, acc=acc, batch_size=100, n_epoch=500, print_freq=5, X_val=X_val, y_val=y_val, eval_train=False, tensorboard=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] Start testing the network ...\n",
      "[TL]    test loss: 0.669836\n",
      "[TL]    test acc: 0.759868\n"
     ]
    }
   ],
   "source": [
    "tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] [*] model.npz saved\n"
     ]
    }
   ],
   "source": [
    "tl.files.save_npz(network.all_params, name='model_old.npz')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] [*] Load model_old.npz SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorlayer.layers.core.DenseLayer at 0x1f3de84df60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.files.load_and_assign_npz(sess=sess, name='model_old.npz', network=network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling TensorRider Using the Generated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import socket\n",
    "import threading\n",
    "from time import ctime,sleep\n",
    "import string\n",
    "\n",
    "remoteImage = np.array([])\n",
    "stream = urllib.request.urlopen('http://192.168.73.73:8080/?action=stream&ignored.mjpg')\n",
    "bytes = bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Image_Refreshing_Thread():\n",
    "    global remoteImage\n",
    "    global stream\n",
    "    global bytes\n",
    "    while True:\n",
    "        bytes += stream.read(1024)\n",
    "        a = bytes.find(b'\\xff\\xd8')\n",
    "        b = bytes.find(b'\\xff\\xd9')\n",
    "        if a != -1 and b != -1:\n",
    "            jpg = bytes[a:b+2]\n",
    "            bytes = bytes[b+2:]\n",
    "            i = cv2.imdecode(np.fromstring(jpg, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "            i = rgb2mono(i)\n",
    "            remoteImage = i.reshape((1,4800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Controlling_Thread():\n",
    "    global remoteImage\n",
    "    addr=('192.168.73.73',51423)\n",
    "    s=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n",
    "    while True:\n",
    "        direction = tl.utils.predict(sess, network, remoteImage, x, y_op, batch_size=None)\n",
    "        msgCtrl_Udp = str(50) + \",\" + str((direction[0] - 3) * 26)\n",
    "#         print(msgCtrl_Udp)\n",
    "        s.sendto(msgCtrl_Udp.encode('utf-8'), addr)\n",
    "        sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "RefreshImageThread = threading.Thread(target = Image_Refreshing_Thread)\n",
    "RefreshImageThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "ControllingThread = threading.Thread(target = Controlling_Thread)\n",
    "ControllingThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
