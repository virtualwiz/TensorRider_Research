{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRider Algorithm Testrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jump to \"LOAD ARRAYS\" if you want to load a NPZ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2mono(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    # mono = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    mono = 0.5 * r + 0.25 * g + 0.25 * b\n",
    "    return mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataArrays(iterator):\n",
    "    X = np.zeros(4800)\n",
    "    y = np.zeros(1)\n",
    "    recordCounter = 0;\n",
    "\n",
    "    for string_record in iterator:\n",
    "        recordCounter += 1\n",
    "\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(string_record)\n",
    "        imageString = (example.features.feature['image'].bytes_list.value[0])\n",
    "        label = (example.features.feature['label'].int64_list.value[0])\n",
    "\n",
    "        image = np.fromstring(imageString, dtype=np.uint8)\n",
    "        image = image.reshape((60, 80, 3))\n",
    "        image = rgb2mono(image)\n",
    "        image = image.reshape((4800))\n",
    "\n",
    "        X = np.vstack((X,image))\n",
    "        y = np.append(y,label)\n",
    "        if recordCounter % 100 == 0:\n",
    "            print(recordCounter,end = '->')\n",
    "\n",
    "    y = y.reshape((recordCounter + 1,))\n",
    "    y = np.round(y / 7) #Downsampling\n",
    "    y = y + 6\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIterator = tf.python_io.tf_record_iterator(path=\"train.tfrecords\")\n",
    "valIterator = tf.python_io.tf_record_iterator(path=\"val.tfrecords\")\n",
    "testIterator = tf.python_io.tf_record_iterator(path=\"test.tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD Datasets from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train...\n",
      "100->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200->300->400->500->600->700->800->900->1000->1100->1200->1300->1400->1500->1600->1700->1800->1900->2000->2100->2200->2300->2400->2500->2600->2700->2800->2900->3000->3100->3200->3300->3400->3500->3600->3700->3800->3900->4000->4100->4200->4300->4400->4500->4600->4700->4800->4900->5000->5100->5200->5300->5400->5500->5600->5700->5800->5900->6000->6100->6200->6300->6400->6500->6600->6700->6800->6900->7000->7100->7200->7300->7400->7500->7600->7700->7800->7900->8000->8100->8200->8300->8400->8500->8600->8700->8800->8900->9000->9100->9200->9300->9400->9500->9600->9700->9800->9900->10000->10100->10200->10300->10400->10500->10600->10700->10800->10900->11000->11100->11200->11300->11400->11500->11600->11700->11800->11900->12000->12100->12200->12300->12400->12500->12600->12700->12800->12900->13000->13100->13200->13300->13400->13500->\n",
      "Val...\n",
      "100->200->300->400->500->600->700->800->900->1000->1100->1200->1300->1400->1500->1600->1700->\n",
      "Test...\n",
      "100->200->300->400->500->600->700->800->900->1000->1100->1200->"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain...\")\n",
    "X_train, y_train = prepareDataArrays(trainIterator)\n",
    "print(\"\\nVal...\")\n",
    "X_val, y_val = prepareDataArrays(valIterator)\n",
    "print(\"\\nTest...\")\n",
    "X_test, y_test = prepareDataArrays(testIterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Save the Loaded Arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"arrays.npz\", xtr = X_train, ytr = y_train, xval = X_val, yval = y_val, xt = X_test, yt = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "npRecall = np.load(\"arrays.npz\")\n",
    "X_train = npRecall[\"xtr\"]\n",
    "y_train = npRecall[\"ytr\"]\n",
    "X_val = npRecall[\"xval\"]\n",
    "y_val = npRecall[\"yval\"]\n",
    "X_test = npRecall[\"xt\"]\n",
    "y_test = npRecall[\"yt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 4800], name='x')\n",
    "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  input: (?, 4800)\n",
      "[TL] DenseLayer  relu1: 2048 relu\n",
      "[TL] DenseLayer  relu2: 2048 relu\n",
      "[TL] DenseLayer  relu3: 1024 relu\n",
      "[TL] DenseLayer  output: 13 identity\n"
     ]
    }
   ],
   "source": [
    "network = tl.layers.InputLayer(x, name='input')\n",
    "# network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
    "network = tl.layers.DenseLayer(network, 2048, tf.nn.relu, name='relu1')\n",
    "# network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
    "network = tl.layers.DenseLayer(network, 2048, tf.nn.relu, name='relu2')\n",
    "# network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n",
    "network = tl.layers.DenseLayer(network, 1024, tf.nn.relu, name='relu3')\n",
    "network = tl.layers.DenseLayer(network, n_units=13, act=tf.identity, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = network.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='cost')\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "y_op = tf.argmax(tf.nn.softmax(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = network.all_params\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   param   0: relu1/W:0            (4800, 2048)       float32_ref (mean: -2.6912639441434294e-05, median: -4.182178145129001e-06, std: 0.08797827363014221)   \n",
      "[TL]   param   1: relu1/b:0            (2048,)            float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   2: relu2/W:0            (2048, 2048)       float32_ref (mean: 7.742324669379741e-05, median: 8.132979564834386e-05, std: 0.08798182755708694)   \n",
      "[TL]   param   3: relu2/b:0            (2048,)            float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   4: relu3/W:0            (2048, 1024)       float32_ref (mean: 1.5352756236097775e-05, median: -2.5950435883714817e-05, std: 0.08791917562484741)   \n",
      "[TL]   param   5: relu3/b:0            (1024,)            float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   6: output/W:0           (1024, 13)         float32_ref (mean: 2.5202449251082726e-05, median: -0.0005437800427898765, std: 0.08768826723098755)   \n",
      "[TL]   param   7: output/b:0           (13,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   num of params: 16140301\n"
     ]
    }
   ],
   "source": [
    "network.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]   layer   0: relu1/Relu:0         (?, 2048)          float32\n",
      "[TL]   layer   1: relu2/Relu:0         (?, 2048)          float32\n",
      "[TL]   layer   2: relu3/Relu:0         (?, 1024)          float32\n",
      "[TL]   layer   3: output/Identity:0    (?, 13)            float32\n"
     ]
    }
   ],
   "source": [
    "network.print_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] Start training the network ...\n",
      "[TL] Epoch 1 of 500 took 4.404456s\n",
      "[TL]    val loss: 863.873876\n",
      "[TL]    val acc: 0.552353\n",
      "[TL] Epoch 5 of 500 took 3.909627s\n",
      "[TL]    val loss: 389.066162\n",
      "[TL]    val acc: 0.522941\n",
      "[TL] Epoch 10 of 500 took 3.868526s\n",
      "[TL]    val loss: 219.092220\n",
      "[TL]    val acc: 0.572941\n",
      "[TL] Epoch 15 of 500 took 3.937354s\n",
      "[TL]    val loss: 141.789698\n",
      "[TL]    val acc: 0.587647\n",
      "[TL] Epoch 20 of 500 took 3.903558s\n",
      "[TL]    val loss: 90.574774\n",
      "[TL]    val acc: 0.600000\n",
      "[TL] Epoch 25 of 500 took 3.908041s\n",
      "[TL]    val loss: 86.699259\n",
      "[TL]    val acc: 0.487059\n",
      "[TL] Epoch 30 of 500 took 3.895489s\n",
      "[TL]    val loss: 67.408201\n",
      "[TL]    val acc: 0.547059\n",
      "[TL] Epoch 35 of 500 took 3.891697s\n",
      "[TL]    val loss: 41.641300\n",
      "[TL]    val acc: 0.602353\n",
      "[TL] Epoch 40 of 500 took 3.928660s\n",
      "[TL]    val loss: 29.993563\n",
      "[TL]    val acc: 0.612941\n",
      "[TL] Epoch 45 of 500 took 3.924129s\n",
      "[TL]    val loss: 25.767626\n",
      "[TL]    val acc: 0.575294\n",
      "[TL] Epoch 50 of 500 took 3.889425s\n",
      "[TL]    val loss: 21.942170\n",
      "[TL]    val acc: 0.572353\n",
      "[TL] Epoch 55 of 500 took 3.918447s\n",
      "[TL]    val loss: 18.113056\n",
      "[TL]    val acc: 0.595294\n",
      "[TL] Epoch 60 of 500 took 3.870389s\n",
      "[TL]    val loss: 17.128665\n",
      "[TL]    val acc: 0.548824\n",
      "[TL] Epoch 65 of 500 took 3.929153s\n",
      "[TL]    val loss: 17.164601\n",
      "[TL]    val acc: 0.520000\n",
      "[TL] Epoch 70 of 500 took 3.888563s\n",
      "[TL]    val loss: 13.700979\n",
      "[TL]    val acc: 0.538824\n",
      "[TL] Epoch 75 of 500 took 3.912940s\n",
      "[TL]    val loss: 12.429821\n",
      "[TL]    val acc: 0.548824\n",
      "[TL] Epoch 80 of 500 took 3.895500s\n",
      "[TL]    val loss: 10.256034\n",
      "[TL]    val acc: 0.557059\n",
      "[TL] Epoch 85 of 500 took 4.077631s\n",
      "[TL]    val loss: 7.987216\n",
      "[TL]    val acc: 0.539412\n",
      "[TL] Epoch 90 of 500 took 3.925258s\n",
      "[TL]    val loss: 8.635416\n",
      "[TL]    val acc: 0.601176\n",
      "[TL] Epoch 95 of 500 took 3.966189s\n",
      "[TL]    val loss: 5.996251\n",
      "[TL]    val acc: 0.589412\n",
      "[TL] Epoch 100 of 500 took 4.080203s\n",
      "[TL]    val loss: 6.182467\n",
      "[TL]    val acc: 0.548235\n",
      "[TL] Epoch 105 of 500 took 3.906837s\n",
      "[TL]    val loss: 6.401775\n",
      "[TL]    val acc: 0.560000\n",
      "[TL] Epoch 110 of 500 took 3.950892s\n",
      "[TL]    val loss: 5.498645\n",
      "[TL]    val acc: 0.585882\n",
      "[TL] Epoch 115 of 500 took 3.895285s\n",
      "[TL]    val loss: 4.984604\n",
      "[TL]    val acc: 0.601176\n",
      "[TL] Epoch 120 of 500 took 3.924832s\n",
      "[TL]    val loss: 4.642945\n",
      "[TL]    val acc: 0.585882\n",
      "[TL] Epoch 125 of 500 took 3.964665s\n",
      "[TL]    val loss: 4.723767\n",
      "[TL]    val acc: 0.590000\n",
      "[TL] Epoch 130 of 500 took 3.912163s\n",
      "[TL]    val loss: 5.422912\n",
      "[TL]    val acc: 0.481176\n",
      "[TL] Epoch 135 of 500 took 3.887734s\n",
      "[TL]    val loss: 4.118026\n",
      "[TL]    val acc: 0.635882\n",
      "[TL] Epoch 140 of 500 took 3.925652s\n",
      "[TL]    val loss: 4.442916\n",
      "[TL]    val acc: 0.567059\n",
      "[TL] Epoch 145 of 500 took 3.962324s\n",
      "[TL]    val loss: 3.841786\n",
      "[TL]    val acc: 0.538824\n",
      "[TL] Epoch 150 of 500 took 3.909041s\n",
      "[TL]    val loss: 3.966819\n",
      "[TL]    val acc: 0.621765\n",
      "[TL] Epoch 155 of 500 took 3.898945s\n",
      "[TL]    val loss: 3.493696\n",
      "[TL]    val acc: 0.618824\n",
      "[TL] Epoch 160 of 500 took 3.915638s\n",
      "[TL]    val loss: 3.518037\n",
      "[TL]    val acc: 0.585882\n",
      "[TL] Epoch 165 of 500 took 3.922286s\n",
      "[TL]    val loss: 3.529907\n",
      "[TL]    val acc: 0.568824\n",
      "[TL] Epoch 170 of 500 took 3.917213s\n",
      "[TL]    val loss: 3.324411\n",
      "[TL]    val acc: 0.578824\n",
      "[TL] Epoch 175 of 500 took 3.918985s\n",
      "[TL]    val loss: 3.021812\n",
      "[TL]    val acc: 0.591176\n",
      "[TL] Epoch 180 of 500 took 3.921381s\n",
      "[TL]    val loss: 3.171145\n",
      "[TL]    val acc: 0.577059\n",
      "[TL] Epoch 185 of 500 took 3.922571s\n",
      "[TL]    val loss: 3.410248\n",
      "[TL]    val acc: 0.576471\n",
      "[TL] Epoch 190 of 500 took 4.000100s\n",
      "[TL]    val loss: 2.979077\n",
      "[TL]    val acc: 0.578235\n",
      "[TL] Epoch 195 of 500 took 3.911779s\n",
      "[TL]    val loss: 2.631376\n",
      "[TL]    val acc: 0.590000\n",
      "[TL] Epoch 200 of 500 took 3.890112s\n",
      "[TL]    val loss: 2.635444\n",
      "[TL]    val acc: 0.588235\n",
      "[TL] Epoch 205 of 500 took 3.901302s\n",
      "[TL]    val loss: 3.382764\n",
      "[TL]    val acc: 0.581176\n",
      "[TL] Epoch 210 of 500 took 3.874068s\n",
      "[TL]    val loss: 2.483791\n",
      "[TL]    val acc: 0.621765\n",
      "[TL] Epoch 215 of 500 took 3.897247s\n",
      "[TL]    val loss: 2.894636\n",
      "[TL]    val acc: 0.626471\n",
      "[TL] Epoch 220 of 500 took 3.954961s\n",
      "[TL]    val loss: 2.875068\n",
      "[TL]    val acc: 0.575882\n",
      "[TL] Epoch 225 of 500 took 3.927076s\n",
      "[TL]    val loss: 3.283522\n",
      "[TL]    val acc: 0.507059\n",
      "[TL] Epoch 230 of 500 took 3.866532s\n",
      "[TL]    val loss: 2.424672\n",
      "[TL]    val acc: 0.614706\n",
      "[TL] Epoch 235 of 500 took 3.892155s\n",
      "[TL]    val loss: 2.853948\n",
      "[TL]    val acc: 0.558235\n",
      "[TL] Epoch 240 of 500 took 4.027613s\n",
      "[TL]    val loss: 2.204669\n",
      "[TL]    val acc: 0.588235\n",
      "[TL] Epoch 245 of 500 took 3.894966s\n",
      "[TL]    val loss: 2.094423\n",
      "[TL]    val acc: 0.621765\n",
      "[TL] Epoch 250 of 500 took 3.915340s\n",
      "[TL]    val loss: 2.842679\n",
      "[TL]    val acc: 0.609412\n",
      "[TL] Epoch 255 of 500 took 3.991174s\n",
      "[TL]    val loss: 2.690565\n",
      "[TL]    val acc: 0.615882\n",
      "[TL] Epoch 260 of 500 took 4.012537s\n",
      "[TL]    val loss: 2.371508\n",
      "[TL]    val acc: 0.618235\n",
      "[TL] Epoch 265 of 500 took 3.885856s\n",
      "[TL]    val loss: 3.100219\n",
      "[TL]    val acc: 0.600000\n",
      "[TL] Epoch 270 of 500 took 3.905391s\n",
      "[TL]    val loss: 2.679238\n",
      "[TL]    val acc: 0.595294\n",
      "[TL] Epoch 275 of 500 took 4.000235s\n",
      "[TL]    val loss: 2.502225\n",
      "[TL]    val acc: 0.584118\n",
      "[TL] Epoch 280 of 500 took 3.935197s\n",
      "[TL]    val loss: 2.764442\n",
      "[TL]    val acc: 0.624118\n",
      "[TL] Epoch 285 of 500 took 3.905543s\n",
      "[TL]    val loss: 3.109362\n",
      "[TL]    val acc: 0.611176\n",
      "[TL] Epoch 290 of 500 took 3.960180s\n",
      "[TL]    val loss: 2.807496\n",
      "[TL]    val acc: 0.612941\n",
      "[TL] Epoch 295 of 500 took 3.930927s\n",
      "[TL]    val loss: 2.788662\n",
      "[TL]    val acc: 0.599412\n",
      "[TL] Epoch 300 of 500 took 3.894863s\n",
      "[TL]    val loss: 2.888286\n",
      "[TL]    val acc: 0.575882\n",
      "[TL] Epoch 305 of 500 took 3.912391s\n",
      "[TL]    val loss: 3.005097\n",
      "[TL]    val acc: 0.616471\n",
      "[TL] Epoch 310 of 500 took 3.927201s\n",
      "[TL]    val loss: 2.760892\n",
      "[TL]    val acc: 0.592353\n",
      "[TL] Epoch 315 of 500 took 3.940934s\n",
      "[TL]    val loss: 2.889025\n",
      "[TL]    val acc: 0.587647\n",
      "[TL] Epoch 320 of 500 took 3.926391s\n",
      "[TL]    val loss: 2.812968\n",
      "[TL]    val acc: 0.632353\n",
      "[TL] Epoch 325 of 500 took 3.921360s\n",
      "[TL]    val loss: 2.926034\n",
      "[TL]    val acc: 0.621765\n",
      "[TL] Epoch 330 of 500 took 3.960054s\n",
      "[TL]    val loss: 3.026827\n",
      "[TL]    val acc: 0.616471\n",
      "[TL] Epoch 335 of 500 took 3.902770s\n",
      "[TL]    val loss: 3.129094\n",
      "[TL]    val acc: 0.581176\n",
      "[TL] Epoch 340 of 500 took 3.890307s\n",
      "[TL]    val loss: 2.755888\n",
      "[TL]    val acc: 0.618824\n",
      "[TL] Epoch 345 of 500 took 3.860162s\n",
      "[TL]    val loss: 3.234567\n",
      "[TL]    val acc: 0.611765\n",
      "[TL] Epoch 350 of 500 took 3.971556s\n",
      "[TL]    val loss: 2.933241\n",
      "[TL]    val acc: 0.632941\n",
      "[TL] Epoch 355 of 500 took 4.038217s\n",
      "[TL]    val loss: 3.026502\n",
      "[TL]    val acc: 0.607059\n",
      "[TL] Epoch 360 of 500 took 4.055579s\n",
      "[TL]    val loss: 3.359482\n",
      "[TL]    val acc: 0.594118\n",
      "[TL] Epoch 365 of 500 took 3.955917s\n",
      "[TL]    val loss: 2.984051\n",
      "[TL]    val acc: 0.595294\n",
      "[TL] Epoch 370 of 500 took 4.126591s\n",
      "[TL]    val loss: 2.868439\n",
      "[TL]    val acc: 0.619412\n",
      "[TL] Epoch 375 of 500 took 4.039864s\n",
      "[TL]    val loss: 2.486546\n",
      "[TL]    val acc: 0.617059\n",
      "[TL] Epoch 380 of 500 took 3.902124s\n",
      "[TL]    val loss: 3.095118\n",
      "[TL]    val acc: 0.622941\n",
      "[TL] Epoch 385 of 500 took 3.880241s\n",
      "[TL]    val loss: 2.711533\n",
      "[TL]    val acc: 0.607647\n",
      "[TL] Epoch 390 of 500 took 3.905714s\n",
      "[TL]    val loss: 3.192484\n",
      "[TL]    val acc: 0.627059\n",
      "[TL] Epoch 395 of 500 took 4.133278s\n",
      "[TL]    val loss: 3.062393\n",
      "[TL]    val acc: 0.609412\n",
      "[TL] Epoch 400 of 500 took 4.044521s\n",
      "[TL]    val loss: 3.129516\n",
      "[TL]    val acc: 0.620000\n",
      "[TL] Epoch 405 of 500 took 4.218684s\n",
      "[TL]    val loss: 3.389078\n",
      "[TL]    val acc: 0.581765\n",
      "[TL] Epoch 410 of 500 took 3.948584s\n",
      "[TL]    val loss: 3.381529\n",
      "[TL]    val acc: 0.608235\n",
      "[TL] Epoch 415 of 500 took 4.029044s\n",
      "[TL]    val loss: 3.156389\n",
      "[TL]    val acc: 0.632353\n",
      "[TL] Epoch 420 of 500 took 4.146320s\n",
      "[TL]    val loss: 3.319457\n",
      "[TL]    val acc: 0.607059\n",
      "[TL] Epoch 425 of 500 took 4.042072s\n",
      "[TL]    val loss: 3.153706\n",
      "[TL]    val acc: 0.631176\n",
      "[TL] Epoch 430 of 500 took 3.908076s\n",
      "[TL]    val loss: 2.787358\n",
      "[TL]    val acc: 0.622941\n",
      "[TL] Epoch 435 of 500 took 3.846850s\n",
      "[TL]    val loss: 2.860609\n",
      "[TL]    val acc: 0.631765\n",
      "[TL] Epoch 440 of 500 took 3.911102s\n",
      "[TL]    val loss: 3.276975\n",
      "[TL]    val acc: 0.596471\n",
      "[TL] Epoch 445 of 500 took 3.878676s\n",
      "[TL]    val loss: 2.828387\n",
      "[TL]    val acc: 0.593529\n",
      "[TL] Epoch 450 of 500 took 4.022035s\n",
      "[TL]    val loss: 3.393316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL]    val acc: 0.592353\n",
      "[TL] Epoch 455 of 500 took 4.093303s\n",
      "[TL]    val loss: 2.964803\n",
      "[TL]    val acc: 0.627059\n",
      "[TL] Epoch 460 of 500 took 3.882324s\n",
      "[TL]    val loss: 3.238923\n",
      "[TL]    val acc: 0.618235\n",
      "[TL] Epoch 465 of 500 took 3.884013s\n",
      "[TL]    val loss: 3.103954\n",
      "[TL]    val acc: 0.639412\n",
      "[TL] Epoch 470 of 500 took 4.045758s\n",
      "[TL]    val loss: 3.018092\n",
      "[TL]    val acc: 0.622353\n",
      "[TL] Epoch 475 of 500 took 3.861598s\n",
      "[TL]    val loss: 2.979404\n",
      "[TL]    val acc: 0.625882\n",
      "[TL] Epoch 480 of 500 took 3.894463s\n",
      "[TL]    val loss: 2.808971\n",
      "[TL]    val acc: 0.618235\n",
      "[TL] Epoch 485 of 500 took 3.927776s\n",
      "[TL]    val loss: 4.200396\n",
      "[TL]    val acc: 0.581765\n",
      "[TL] Epoch 490 of 500 took 3.896533s\n",
      "[TL]    val loss: 3.055398\n",
      "[TL]    val acc: 0.620000\n",
      "[TL] Epoch 495 of 500 took 3.915275s\n",
      "[TL]    val loss: 2.767656\n",
      "[TL]    val acc: 0.662353\n",
      "[TL] Epoch 500 of 500 took 3.853869s\n",
      "[TL]    val loss: 2.701859\n",
      "[TL]    val acc: 0.637647\n",
      "[TL] Total training time: 1986.586011s\n"
     ]
    }
   ],
   "source": [
    "tl.utils.fit(\n",
    "    sess, network, train_op, cost, X_train, y_train, x, y_, acc=acc, batch_size=50, n_epoch=500, print_freq=5, X_val=X_val, y_val=y_val, eval_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] Start testing the network ...\n",
      "[TL]    test loss: 4.745813\n",
      "[TL]    test acc: 0.545230\n"
     ]
    }
   ],
   "source": [
    "tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] [*] model.npz saved\n"
     ]
    }
   ],
   "source": [
    "tl.files.save_npz(network.all_params, name='model.npz')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
